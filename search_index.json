[
["index.html", "Let’s program in R (“Introduction to programming language R” course book) Foreword", " Let’s program in R Damir Pintar 2019-01-21 (“Introduction to programming language R” course book) ** NOTE: During the current academic year, the “Introduction to Programming Language R” is revised chapter by chapter. After the subject is New chapters will be dynamically added to this HTML document. If you need to have access to all the materials, contact the author of the tutorial at damir.pintar@fer.hr** Foreword This tutorial is based on interactive lessons used in the “Introduction to Programming Language R” at the Faculty of Electrical Engineering and Computing at the University of Zagreb. But the topics discussed here are not only useful to the students of the mentioned faculty - knowledge of the language R will be good both in academia and in the business world. Although R is known as a “programming language made up of statisticians, for statisticians” and is most often associated with the field of data science within which it is used for complex statistical analysis and data mining, it can be very useful for tasks related to the management of smaller or larger data at tasks that are not necessarily strictly oriented to advanced analytics. Namely, popular graphical tools with their interactive tabular presentation are very intuitive and excellent for simpler tasks, but as the need for more complex tasks appears, they quickly lose their efficiency and simplicity; on the other hand, the interactive program approach offered by R is initially somewhat more demanding but long-term highly cost-effective because complex tasks can be dealt with in an efficient, consistent and insightful way. For this reason, in the business world there is a clear shifting trend from classic GUI office tools to platforms with better support for more complex calculations and the creation of attractive visualizations. This is evidenced by a strong increase in the popularity of R language and other platforms with similar approach to data analysis. The aforementioned popularity of R language results in an increased need for learning resources, which are not currently very much present in Croatia. This coursebook will try to make learning R as easy and interesting as possible through its “learning through examples” approach. Emphasis will be placed primarily on mastering R as a programming language. For this reason, the initial chapters will focus on “programmatical aspects”, followed by a review of available tools presumed to be useful for the widest set of readers - tools for data gathering, extracting useful information, and visualizations. Since R is a domain-oriented language, R’s role in its support for statistical analysis will be reviewed followed byexamining selected machine learning methods and their applications. Although there will be enough information to put all the presented methods into context, the idea of this textbook is not to teach readers statistics nor deeply enter the field of machine learning - the intention of the author is to intrigue readers to continue exploring this interesting area, adequately armed with platform knowledge that will enable all new knowledge is immediately practically applied in further research. "],
["introduction.html", "1 Introduction 1.1 What is R? 1.2 Installing Software Support 1.3 Overview of the development interface * RStudio * 1.4 How to use this coursebook?", " 1 Introduction 1.1 What is R? 1.1.1 General facts about R Programming language R came from the programming language S , developed for the needs of * Bell Telephone * Laboratory owned by * AT &amp; T * corporation. It was designed as an internal statistical analysis tool . The basic philosophy of the S language (inherited by the programming language R) was domain orientation - ie facilitating work with data analysts without the need to adapt conventions to traditional programming languages. Language S gained significant popularity in business analysts and statisticians within the 80s and 90s, but is now only available through a commercial variant called S-PLUS . The programming language R was created at the University of Auckland (NZ), modeled on S, and is released under the GNU Open Code Code . The standard distribution of the R programming language consists of: “core” R, with basic functions and so called “core” base package that provides basic functionality a collection of additional packages (“base” - * base * and “recommended” - * recommended *) for data management, visualization and statistical analysis We must not ignore the excellent integration of R with a rich repository called CRAN (Coprehensive R Archive Network *) that enables fast and easy installation of any packet from that repository, after which it becomes part of the local R installation. Since the R community is extremely strong in the development of new packages, often after the introduction of new experimental methods and access to data analysis, CRAN can quickly offer packages that implement them. Also, strong and continuous enthusiasm of the R community for the enhancment of existing R elements alleviates or eliminates a large number of detected language deficiencies. R is therefore often able to compare it with a “do it yourself” project where, after getting acquainted with the supplied “factory” components (in this case basic functions and packages), the user begins to adapt his development environment by choosing a package that exactly matches his needs and preferences. Creativity and flexibility in using R is considered to be of great advantage, although it results in a certain informality and liberal approach to programming is occasionally not favored by users familiar with strict and formal programming frameworks with a clear set of guidelines and rules to be followed Despite the exceptionally high acceptance of the R language for data analysis and the variety of options offered to the user, it is necessary to be aware of its limitations: R intensely uses RAM which has been considered a serious restriction for a long time; By increasing the capacity of modern hardware systems, this limitation is much more acceptable today, and there are also numerous packages that rationalize the use of memory. Still, the fact remains that R quickly eats the RAM of our computer, though it is often the result of the neglect or ignorance of a developer who has not adopted the “R” mode of programming well enough. R is quite unconventional so the learning curve is initially steeper, especially for programmers accustomed to standard conventions of other programming languages. On the other hand, if viewed long-term, programming in R is quite simple since most of the complex tasks are abstracted into high-level functions that transparently perform low-level operative tasks. It is often said that R is more oriented towards the goal we want to achieve and less detail about the way we reach it. R is not a “fast” language ; although it is a language that is expected to work over large data sets, R is not optimized for performance speed or even for parallelism; although there is a great deal of effort to implement virtually all key routines in C which prevents significant slowdowns, and there are a number of packages that allow multiple execution of the R program, the fact remains that R is not designed to be executed as quickly as possible ; If speed is a priority, it is often necessary to look for alternative solutions - which is why it is often said that R is primarily a research language, not a production language. R is primarily intended for interactive work , i.e. performing a series of machine instructions that are dynamically enrolled and executed with the help of a program console. This is tailored to the standard process of data analysis where the analyst can download data, clean it, transform, develop models, test, etc. with constant feedback from a computer an overview of intercepts , adapt the analysis process to current findings etc. This does not mean that programming language can not be programmed in a classical “procedural” way by developing algorithms encapsulated in functions that automatically perform their tasks after call, but the fact is that the efficiency of R is exactly reflected in interactive work. This principle is also transmitted to the teaching of R; programming language R is much easier to learn with interactive approaches by performing specific tasks, experimenting with data sets, accessible methods, and so on, rather than using a “classic” approach by designing scripts that implement some low-level jobs. 1.1.2 R alternatives Programming Language R is a popular but not the only solution for interactive data analysis and statistical programming. Below we will give a brief overview of some of the more popular technologies and solutions used today for this purpose, with a brief comparison and a review of the advantages and disadvantages compared to language R. SAS and SPSS - SAS (* Statistical Analysis System , developed by SAS Institute ) and SPSS ( Software Package for Statistical Analysis , developed by IBM *) are two different software packages that we put under the same paragraph primarily because they are commercial tools, ie tools that require full payment for their full functionality. Similarly, SAS and SPSS are relatively easy to learn and their functionality is largely based on carefully designed user interfaces. These tools emphasize efficiency and are an excellent option for large companies looking for a consistent, robust solution for their analytics, not bothered by the commercial nature of such solutions. Weka and Orange - Weka (* Waikato Environment for Knowledge Analysis, developed by Waikato University in New Zealand) and * Orange * (deep data analysis software developed at the University of Ljubljana) are free software for exploratory data analysis and data mining that base their functionality on relatively simple graphing interfaces and visual programming approach. These solutions are very good for users who are not too demanding in terms of the flexibility and complexity of their analysis because they allow the implementation of defined steps in the analysis process in a very accessible and clear way. This does not mean that these tools can not do more complex analysis, but they are still more suited to analyzes through the predefined functionality of the provided graphical interface. Python (Numpy / Pandas / Scikit) - in the last couple of years, Python is the most serious competitor of language R, primarily because Python is a very popular programming language with a very similar process approach to data analysis compared to one used by language R. The discussion of which language to choose is very common in the field of data science, usually without a clear final conclusion. It’s easy to make sure that the differences are in fact miniscule - while R is strongly domain-oriented and emphasis is placed on ease and ease of use with a wide range of available overlapping packages to enable the user to choose the one that best suits him, Python emphasizes the rigid formal structure and principle “for one job one way of doing”. Therefore, it could be said that R is somewhat more suitable for “data research” while Python’s advantage is easier to develop and integrate analytical modules in a production environment, especially if the above environment is already implemented in Python. But with the strong development of both languages and the mutual overlappiong of functionality, this distinction becomes less relevant - it is no longer a problem to integrate R scripts into existing systems regardless of the platform they are running on, and the Python community is developing its versions of popular packages from R that faithfully map their functionality. Ultimately, it can be said that the initial choice between these two alternatives is not so important - the approach they use is so similar and the functionality sharing is so pronounced that learning a single language introduce all of the major concepts from the other so data scientists often decide to master both languages, to more easily adapt to a large number of environments in which they must conduct their analyses. 1.2 Installing Software Support Installing the R language support software is pretty simple, especially if the recommended development interface RStudio is used as a platform. This is not the only option - one of the popular alternatives is the multilingual platform Jupyter Notebook which offers its own R support. Readers are ecnouraged to explore all available options and choose the final selection of the interface that personally matches their needs best, but this coursebook will focus on RStudio mainly because of a clear, easy-to-view interface, easy installation and a very rich support for a variety of functionalities - from installing new packages, easy retrieval of documentation, creating visualizations and publishing reports. Notions that will be discussed below will mostly assume that you have chosen an interface RStudio . To successfully set up a development platform, two things need to be installed language distribution R development interface RStudio It is recommended to use the latest available versions. At the time of writing this document, they are R 3.3.2 and RStudio 1.0.136 . If these versions differ from those on your computer, there will probably be no problem if the version numbers are higher than the above; otherwise, their upgrade is recommended. The procedure for installing this software on the operating system * Microsoft Windows * will be described below. If you are working on some other operating system, such as some * Linux * distribution or * Mac OS *, the procedure is somewhat different, but still not too complex - it’s enough to follow the instructions on the websites mentioned below that are a platform-oriented use. To find the software mentioned in the search engine, type the following terms: download R * download RStudio * In both cases, you will get links to the executable files that you have to run to install the software on your computer. In the case of R, this can be a file * R-3.3.2-win.exe * (exact numbers may differ). In the interface * RStudio * you can see more options - choose a free &quot;* desktop *&quot; version. Commercial versions have some additional functionalities that are mostly oriented to use in professional multi-user environments and are not essential to normal work. You can run the executable files and let the wizard install all the necessary components on your computer. It is recommended to accept all of the nominal options except the installation folder - instead of the subfolder * * Program Files * “it is better to install R directly in the root folder (eg” * C:\\R\\R-3.3.2 * “). This way, it will be easier to find the currently installed version and eventually update it later. For the same reason, it is recommended that * RStudio * be installed in the folder”* C:\\R\\RStudio *&quot;. If you do not have the option or you do not want to choose these folders, you can define some other or retain the default options - this choice should not ultimately affect the further work. After installing the interface * RStudio * it is enough to simply run it with the help of the created shortcut on the workbook (or alternatively, with the help of the executable file * RStudio.exe * in the selected installation folder). After launch, the application should look similar to the following image: Figure 1.1: RStudio interface layout If there are any problems, make sure that you have completed all of the installation steps listed above. Below we will deal with the details of the interface shown. 1.3 Overview of the development interface * RStudio * Let’s look at the interface * RStudio *. We see it being divided into three windows - the left part is the “work area” and the program code is entered into it. On the right, there are auxiliary windows that show different things, depending on the card selected; In the upper right hand corner, we can see, among other things, what is currently in our working environment (which is empty at the beginning) and the history of the commands that we have executed. The bottom part serves to display documentation, files, installed packages, visualizationa, etc. 1.3.1 Interactive console Let’s go back to the left part of the interface, especially the so-called. “interactive console”. Namely, by its nature, R is an “interpreter language” in the sense that commands are immediately interpreted and executed. Though it is possible to create larger scripts that are then executed “all at once”, working with the R language often comes down to the command-by-command principle. This is precisely why we are talking about “interactive program data analysis” - the analyst is “programming” by entering commands and can at any time study the interminent results obtained and decide on further steps. Let’s see how the interactive console works. With the help of a keyboard, you can type a simple math expression - eg 3 + 2 and press the * ENTER * key. We will see that R will immediately deliver the result - we can use it as a calculator! For mathematical expressions that are not simply “typed” we need to use functions. Thus, for example, a square root can be calculated using the sqrt () function. Let’s try typing sqrt(10) in the console and press * ENTER *. R again shows the result immediately. At this time, the screen should look like the next picture. Figure 1.2: R kao kalkulator One of the problems of using this kind of R is the mixing of commands and results, and the history of a string of commands becomes more and more cluttered as we use the console to move everything “lower and lower”. Likewise, if for some reason the command that we execute results in a mistake we are trying to correct, the console becomes “dirty” as mistakes are mixed with error reports, so any slightly more complicated procedure we want to run becomes “torn” and hard to interpret. This is why analysts often use so-called R scripts&quot; that allow you to visually distinguish the commands we want to execute from the console itself, but with the ability to easily transfer them in the console, where we then look at the result. 1.3.2 Writing R scripts In the toolbar, select File -&gt; New File -&gt; R Script (or press the CTRL + SHIFT + N key combination). We see that the “working area” on the left is divided into two parts. The upper part represents the space for our “script” - actually the series of commands we want to execute - while the interactive console now occupies the lower part of the work surface. If we want, we can tweak the size of these (and other windows) by moving the boundaries, but for now it’s important to have a look at the scripts and consoles. Write two commands in the script script - the first one should be print(' hello!') And underneath it a simple mathematical expression 3 + 4. Return the cursor to the first line and press the CTRL + ENTER key combination. If we have correctly followed these steps, the command at the cursor site will automatically be copied to the interactive console and executed. The cursor will now be the next command that you can do with CTRL + ENTER. The screen should now look similar to the next image. Figure 1.3: R script This is actually the usual way of working in R-language - entering the script space into commands that we then execute by automatically copying them into the console. If something is wrong with the command, we can easily modify it and perform it again. If you want to execute a block of commands, select them by clicking and dragging and then press the CTRL + ENTER key. Scripts can be expanded with comments (starting with the # character that R interprets as ’ ignore this line’). We can save our scripts under the chosen name on the hard disk. But we can go one step further. Though the R scripts are quite adequate for comfortable work in the R language, there is an additional technology that gives us even more flexibility in working with the R - * R Markdown *. 1.3.3 * R Markdown * Writing R scripts is very similar to the classic concept of “programming” - we write program commands that are usually executed sequentially, and optionally we add comments for the purpose of the documentation. But since a final step of data analysis usually involves formulating reports that will adequately display the results obtained, the RStudio interface supports technology that provides an effective combination of programming and structured documentation on the principle of “interactive notebooks”. The analyst can write “pure” text, optionally with formulas, images, and changes in the size and nature of the text font, and then can see both the executable code together and its results together, in a format resembling a notebook (in fact, RStudio recently introduced the R Notebook which works very similar to usual “interactive notebook” technologies). This technology is easiest to demonstrate through the examples - in the toolbar, select File -&gt; New File -&gt; R Markdown ... and in the next window choose an arbitrary title (eg &quot;Testing&quot;), optionally add the author’s name, and choose one of the options for final form of report (HTML is recommended due to the lowest dependency on additional packages). Unlike the R script, which will initially appear empty, with RMD R will create a “filled” document. This is done in this way for the simple reason that the user gets an easily modifiable template with all the basic elements included as a reminder. We will delete most of this template for our needs - everything after the initial header, ie below the second sign of ---. Then we can write any text below. With the #, ##, ### etc tags we can set the title of a certain section (they are now not commentaries because this is not R code!), While *and **signs in front and back we select the italic or bold font which weall appear in the final report. This is so-called. pure &quot;* markdown *&quot;, ie plain text that can be converted into formatted text with the help of additional tools, if desired. When we want to incorporate the program code into our “report”, we have to create so-called &quot; code chunk&quot;. This can be done by selecting Insert -&gt; R on the toolbar or using the CTRL + ALT + I keys. Notice that the chunk begins and ends with a specially selected string of characters - three “backticks”. Likewise, the beginning of the chunk has description of certain parameteres in the opening brackets, the most important of which is the programming language we will use. In this textbook, we will almost exclusively use the language R, although other languages may be used if they are installed on the platform running * RStudio *. The code chunk behaves the same as the standard R script - we can enter commands and execute them. The difference is just that - if you want - the results can be seen immediately in the * R Markdown * document itself. If this option is not what we want, we can turn it off (click on the gear in the toolbar and select Chunk output in console) but in general we prefer to have the results right below the code that created them, notebook-style. If we follow the instructions, the screen may look similar to the following image: Figure 1.4: R Markdown document We can now try to create a “report” from the current document. First, we must save it under a particular name (eg Testing.rmd), and then we can click on the Knit button to convert the document from pure text to an HTML file. R Markdown documents are much more powerful than it may seem judging by the elements that have been presented so far. Chunks can have plenty of different parameters which influence their behavior. The output format can be PDF, DOCX as well as other forms such as slides of various technologies, books intended for mobile devices, interactive web applications etc. The coursebook you are reading is actually nothing more than a series of RMD files converted into the appropriate form you are currently using. As we will explain in the next chapter, RMD files are also the main way for you to use this coursebook effectively and try out the examples and tasks that follow. The universality and flexibility of technology * R Markdown * is exceptionally great, and is very well accepted by the R community. 1.4 How to use this coursebook? The basic idea of this coursebook is “learning through application”. Therefore, the lessons below will not use too many examples, but rather the reader will be encouraged to try out each new concept by solving a series of easy or intermediate tasks. Each chapter that follows has an accompanying “workbook”. Simply put, it is an RMD file that contains all the examples from the lecture, accompanied by a concise text for easier reference to the concepts being dealt with. The basic idea is that the reader reads the coursebook and solves the workbook in parallel, looking at the task solution only after it is solved within the programming tool. Some tasks will require simply removing the # sign (meaning “comment”) from the start of the command and executing it. In spite of the trivial approach, in this way, however, the reader is encouraged to independently test the command rather than just look at its result. Other tasks will require a bit more engagement. Finally, after each lesson there is a series of “Exercise Tasks” that will not have the solution and which will represent a kind of evaluation milestone of all the concepts of the lesson. Readers are strongly suggested solving all of the examples and tasks before moving on to the next lesson, as the lessons that follow presume the well-accepted knowledge of all the topics that are discussed previously. Of course, the textbook can be read without the above-mentioned “interactive” approach. Task solutions reveal the correct method of accessing the problem, and most of the commands are accompanied by a print that the user would get on the screen by executing them. Nevertheless, the coursebook author’s attitude is that programming languages can not be taught by reading, and that the extra effort to try all, even the simplest, concepts is ultimately very rewarding. Let’s get to know the concept of workbooks more closely. First you need to find and open a workbook that matches the lesson you are reading. It is easy to recognize by the appropriate number of lessons - the notebook for this lesson is named 01_Introduction_RB.Rmd. It is recommended that all workbooks that you plan to use are copied somewhere to the local computer together with all the accompanying files. As stated previously, the workbook will typically contain all the program code in the lesson to which it refers, but only enough pieces of text sufficient for easier understanding. If you read this text directly from the workbook and not as part of the tutorial, you can see that the entire previous lesson is missing; this is because the introductory steps described in it relate to the concepts that need to be adopted before using the workbook. If you have not passed them, it is a good idea to go back and pass them, and then continue with the following examples and tasks. Workbooks differ in Examples and Exercises. Examples are usually only required to be executed. Exercises on the other hand are expecting some changes or entering a new program code. As said, the tutorial will set far more emphasis on tasks. An example might look like this: Example - a few simple commands R of programming language 3 + 2 # adding log (10) # natural logarithm! log10 (10) # this is a base-10 logarithm! By the way, we comment with the &quot;#&quot; sin (0.5 * pi) # pi is one of the built-in constants ## [1] 5 ## [1] 2.302585 ## [1] 1 ## [1] 1 You can execute the commands from the examples individually, or the entire chunk at once with the CTRL + SHIFT + ENTER key combination. No modification of the code is necessary (although it is often not bad to experiment with the given commands!). Tasks on the other hand always require a certain - even minimal - intervention. Zadatak 1.1 - commands for checking and editing workbooks # Make the following commands by removing the comment character #getwd () # folder in which we are currently working #setwd (&quot;.&quot;) # Here we can specify a new working folder if desired getwd() # directory in which we are currently working setwd(&quot;.&quot;) # Here we can specify a new work directory if desired The exercise will often refer to the just introduced concept. Eg. it is convenient to note that, although language R supports an operator = for assigning a value to a variable, it is recommended to use the &lt;- operator for that purpose, which is somewhat more “R-like”. Also, note that R supports the so-called. * autoprint *, ie always prints the result of the last command on the screen. This means that if we create a new variable x in the snippet and want to print it on the screen, we do not have to put print(x)as the last command, but just x. Let’s try this in the exercise. Zadatak 1.2 - Assignment operator # put `5` in the variable` x` # then print the variable `x` on the screen x &lt;- 5 x ## [1] 5 Now that we are well acquainted with the work platform, we can begin by learning the basic elements of the R programming language. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["tipovi.html", "2 Basic data types and operators 2.1 Basic data types 2.2 Operators 2.3 Missing, unknown, and non-existent values Exercises", " 2 Basic data types and operators “Basic” or “primitive” types of data are the underlying building blocks of programming languages. They are basically embedded mechanisms that allow storing basic information - most commonly of logical, numeric, or character nature. Most programming languages use the same or very similar methods of storing such information, which means that they implement similar basic data types - the difference is often in details such as the actual type name, the nominal number of bytes, etc. In any case, the most common first step in learning a new programming language is to know the basic types of data that it supports. The next thing that may interest us is the language syntax, that is, the way we write commands that language interpreters can understand and execute. The R language in its syntax follows similar conventions seen in languages such as Python, Ruby, or Java, of course with certain specifics. Some basic syntax rules are: each command must, as a rule, go to its own line, but the indentation of commands is not as important as placing a point-to-end commands; defining blocks with bracketed brackets; we do not have to define the types of variables in advance since their type will be inferred by the assigned value; comments start with #; etc. Syntax will be best learned through examples - by learning the elements of the language syntax rules often become intuitively clear. It is best to start with simple functions and operators, as we will do in this lesson. We will end the lecture with a debate on the so-called. “missing” or “non-existent” values. Since R has its own way of defining this type of value, we will immediately clarify the way in which those in R are implemented so that in the following lessons the readers are prepared to easily manage such values (which often occur in work with actual data sets). 2.1 Basic data types R offers six basic data types: tip primjeri logical TRUE, FALSE ili T, F integer 2L, 5L, 123456789L double 4, 6, 3.14, 2e5 complex 5 + 2i, 7 + 1i character &quot;A&quot;, &quot;B&quot;, &quot;Pero&quot;, &quot;ABCDEFGHijklmnoPQRSTUVwyz&quot; raw as.raw(2), charToRaw(&quot;1&quot;) Some remarks: integer and real types are often treated together as a numeric type (although this is not entirely consistent!) complex type must have a declared imaginary constant even if it is 1 (2 + i is not a good record!) The type of “raw” bytes is relatively rarely used Checking whether a variable is of a certain type can work with the is.&lt;type_name&gt; function. We will try this in the next exercise. Before we begin with solving, we introduce one new feature: in exercises where we print more things on the screen, it is useful to visually separate the different print segments so that we can easily understand which part of the code is referenced. For this purpose, we will use the cat(&quot;-----------\\n&quot;) command that simply prints a dash of the line and goes to the new line. We could also use the print() function, but it always starts printing with the element index while the cat command is a “raw” print, which in this case is more appropriate. Zadatak 2.1 - checking data types #try the following commands: #is.logical(FALSE) #is.integer(2L) #is.double(1.11) # perform the following checks: # is 5L numeric? # is 3.14 numeric? # is &quot;ABC&quot; character? # is 4 + 2i complex? # is 5 integer? is.logical(FALSE) is.integer(2L) is.double(1.11) cat(&quot;-----------\\n&quot;) is.numeric(5L) is.numeric(3.14) is.character(&quot;ABC&quot;) is.complex(4 + 2i) is.integer(5) ## [1] TRUE ## [1] TRUE ## [1] TRUE ## ----------- ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] FALSE Did you notice anything unusual in these checks? Try to explain the obtained results. Type some variables or constants we can retrieve with the typeof orclass function. The difference between them is the following: typeof - fetches&quot; primitive&quot; or “basic” type ( integer, double) class - ‘object type’, actually the value of class attribute Zadatak 2.2 - data type retrieval # print the types of the following constants: TRUE, 2L, F, 3.14, &quot;ABC&quot; # print the classes of the same constants. Are there any differences? typeof(TRUE) typeof(2L) typeof(F) typeof(3.14) typeof(&quot;ABC&quot;) cat(&quot;-----------\\n&quot;) class(TRUE) class(2L) class(F) class(3.14) class(&quot;ABC&quot;) ## [1] &quot;logical&quot; ## [1] &quot;integer&quot; ## [1] &quot;logical&quot; ## [1] &quot;double&quot; ## [1] &quot;character&quot; ## ----------- ## [1] &quot;logical&quot; ## [1] &quot;integer&quot; ## [1] &quot;logical&quot; ## [1] &quot;numeric&quot; ## [1] &quot;character&quot; Data can be explicitly converted from one type to another using the function as.&lt;type_name&gt;: Zadatak 2.3 - conversion of data types # perform the following conversions and print the result # 2.35 to integer # TRUE to ntomeric # 100L to character # 2.35 to character # 2e2 to character # 0 to logical # 2.75 to logical as.integer(2.35) as.numeric(TRUE) as.character(100L) as.character(2.35) as.character(2e2) as.logical(0) as.logical(2.75) ## [1] 2 ## [1] 1 ## [1] &quot;100&quot; ## [1] &quot;2.35&quot; ## [1] &quot;200&quot; ## [1] FALSE ## [1] TRUE R will implement implicit conversion if possible: Zadatak 2.4 - implicit conversion # Write the following phrases and print the results: # arithmetic operator between logical and numeric variables # arithmetic operator between integer and numeric variables # logical operator negation (!) applied to numeric variable # arithmetic operator between logical and numeric variables TRUE + 5 # arithmetic operator between integer and numeric variables 5L + 3.14 # logical operator negation (!) applied to numeric variable !25 ## [1] 6 ## [1] 8.14 ## [1] FALSE Implicit conversion will only be performed if it is meaningful - eg an arithmetic operator between the character and numeric variables will result in a mistake 2.2 Operators As in other programming languages, R permits the use of operators in terms. Some of the more frequently used operators are: arithmetic +, -, *, /, **, %% (modulo), %/% comparison &lt;, &lt;=, &gt;, &gt; =, ==, != logical ! (negation), &amp;&amp; (scalar AND), || (scalar OR), &amp; (vector AND), | (vector OR) assignment &lt;- or= Zadatak 2.5 - operators # try the `5 / 2` and` 5%/% 2` expressions # check how much &quot;square of 17&quot; and &quot;the remainder of 101 divided by 12&quot; are # check what is the result of the following expressions: `17 &gt; 13`,`!TRUE`, `5 &amp;&amp; 0`,`0. || 2` # try the `5 / 2` and` 5%/% 2` expressions 5 / 2 5 %/% 2 cat(&quot;-----------\\n&quot;) # check how much &quot;square of 17&quot; and &quot;the remainder of 101 divided by 12&quot; are 17 ^ 2 101 %% 12 cat(&quot;-----------\\n&quot;) # check what is the result of the following expressions: `17 &gt; 13`,`!TRUE`, `5 &amp;&amp; 0`,`0. || 2` 17&gt; 13 ! TRUE 5 &amp;&amp; 0 0. || 2 ## [1] 2.5 ## [1] 2 ## ----------- ## [1] 289 ## [1] 5 ## ----------- ## [1] TRUE ## [1] FALSE ## [1] FALSE ## [1] TRUE Logical values and comparison operators will most commonly be used with so-called “Conditional Execution of Commands”, known from other programming languages as the “IF ELSE” commands. In R, its syntax looks like this: if (expression) {block} else {block} Let’s try this command on the following task: `r zadHead(“conditional execution”) ** # Write a command that performs the following: # &quot;if 100 is an even number print &#39;Success!&#39;&quot; if (100 %% 2 == 0) print(&quot;Success!&quot;) ## [1] &quot;Success!&quot; We have noted above that we have two types of logical operators for “and” and “or”. We will explain the difference later, for now it is enough to say that we are almost exclusively using &amp;&amp; i || (“C++” operators!) with the conditional execution of commands or program loops. Likewise, we have already mentioned that R offers two assignment operators, &lt;- and=. Let’s see if there are anz obvious differences between them. Zadatak 2.6 - assignment operators # create variables x and y and assign the number 5, one with the operator &lt;-, # the other with the operator = # print variables on the screen # Do you notice any difference between these two operators? x &lt;- 5 y = 5 x y ## [1] 5 ## [1] 5 As we could testify, there is no noticeable difference between operator &lt;- and=. There are some minor differences, but they do not have any impact on the normal use of this operator in practice. In the literature, both versions can be seen for assigning values, but we will primarily and consistently use &lt;-, mostly because the code is visually more distinctive than the other programming languages. When assigning, we keep in mind that on the left there is so-called “left value” (lvalue). This is interpreted in the programmer’s sense as “something in which the calculated value can be stored”. x + 1 &lt;- 2 # error !!!] As a rule, in R as lvalue we can most commonly see a variable, though sometimes there may be a function call. This perhaps initially confusing feature will be clarified later. Naming the variables mostly follows the rules from other programming languages - letters, numbers, subfolders, and points are allowed. The first symbol must be a letter or a dot. .myVarijable &lt;- 5 #OK my.Variable &lt;- 5 #OK _myVariable &lt;- 5 # not OK 123Variable &lt;- 5 # not OK In practice for complex name variables we need to select one of the following conventions: myVariable &lt;- 1 # camelcase my_Variable &lt;- 2 # underscore seperation or my.variable &lt;- 3 # point separation It is important that we do not mix conventions in a program code, ie, after the selection, be consistent. If we insist on strange names using special characters, then we have to put them under the so called “left single apostrophes” (* backticks *): Zadatak 2.7 - variable name with special characters # Enter an arbitrary name with special characters inside the left apostrophes # and print the value of the variable # `` &lt;- 2 `!% ^$*@ __ =` &lt;- 2 `!% ^$*@ __ =` ## [1] 2 Such a way of naming variables is not too useful in practice, but has its purpose - since the operators in R are actually functions (whose names are literally +, ** etc.) using left backticks we can directly reference them in their the original form, which can be very practical in the so-called functional programming (which we will talk about in one of the future lessons). Assigning values to new variable names we actually create new variables in the working environment (called the “global environment”“). All variables we have created so far can be seen with the ls() function. If we want to delete some variables, just give their names in the call function rm() (eg rm (x, y, z)). To delete * all * variables from the working environment, we use the call rm(list=ls ()), but we have to be careful (no”undo&quot;!). Zadatak 2.8 - printing and deleting global environment variables # print all of the global environment variables that have been created so far # delete some of the above-written variables - eg rm(x, y, z) # list all remaining variables # delete ALL variables from the global environment # (cautiously with this call in practice!) # Make sure the global environment is empty # print all of the global environment variables that have been created so far ls() # delete some of the above-written variables - eg rm(x, y, z) # list all remaining variables rm(x, y) ls() # delete ALL variables from the global environment # (cautiously with this call in practice!) # Make sure the global environment is empty rm(list=ls()) ls() Finally, whenever we need help with some function, we have the following options available: write only &lt;function_name&gt; (without parenthesis with parameters) and press - if the function is written in R (and not just proxy to a C implementation) the screen will get a printout of the original code function Write help (&lt;function_name&gt;) or ? &lt;name_functions&gt; to get a help function page with the list of parameters, examples, and so on. Write example (&lt;function_name&gt;) where we get a list of examples of using the function and the results obtained The following code snippet shows how to use the above methods (due to space savings, we do not show their result). #program code for `ls` function ls # help for `ls` function ?ls # or help(ls) # examples of using the `ls` function example(ls) 2.3 Missing, unknown, and non-existent values In R there are three ways of modeling “non-existent” values: NA - (* not available *) Missing or unknown value of a particular type NaN - (* not a number *) “impossible”number, eg 0 / 0 NULL - non-existent value, literally “nothing” Zadatak 2.9 - working with NA, NaN and NULL # how much is &quot;5 + unknown number&quot; # how much is &quot;5 + non-existent number&quot; # check classes of the following: # NA # arithmetic operation between numeric and NA # NaN # NULL # how much is &quot;5 + unknown number&quot; 5 + NaN # how much is &quot;5 + non-existent number&quot; 5 + NA cat(&quot;-----------\\n&quot;) # check classes of the following: # NA # arithmetic operation between numeric and NA # NaN # NULL class(NA) # logical type is &quot;weakest&quot;! class(5 + NA) class(NaN) class(NULL) ## [1] NaN ## [1] NA ## ----------- ## [1] &quot;logical&quot; ## [1] &quot;numeric&quot; ## [1] &quot;numeric&quot; ## [1] &quot;NULL&quot; Checking missing valuesis similar to checking data types - we use the is.na,is.nan and is.null functions. We have to keep in mind that NaN is a subclass of NA and that NULL is a completely separate class. This is especially important for SQL users - what is NULL in SQL isNA in R and that is what we normally use in practice, whereas NULL has very specific applications and is not so commonly used the program code. Zadatak 2.10 - check NA, NaN and NULL # which of the following is NA? NA, NaN, NULL, &quot;&quot;, 0 # which of the following is NaN? NA, NaN, NULL # which of the following is NULL? NA, NaN, NULL # which of the following is NA? NA, NaN, NULL, &quot;&quot;, 0 is.na(NA) is.na(NaN) is.na(NULL) is.na(&quot;&quot;) is.na(0) cat(&quot;-----------\\n&quot;) # which of the following is NaN? NA, NaN, NULL is.nan(NA) is.nan(NaN) is.nan(NULL) cat(&quot;-----------\\n&quot;) # which of the following is NULL? NA, NaN, NULL is.null(NA) is.null(NaN) is.null(NULL) ## [1] TRUE ## [1] TRUE ## logical(0) ## [1] FALSE ## [1] FALSE ## ----------- ## [1] FALSE ## [1] TRUE ## logical(0) ## ----------- ## [1] FALSE ## [1] FALSE ## [1] TRUE To end, we dedicate some room to a discussion of the NA value, since we will often encounter it in practice. Simply put, if the NA values appear, we can expect the following side effects: Results of arithmetic expressions result in NA values the results of some function call result with NA (unless we specify compensation actions, such as the parameter na.rm = T which actually means ‘ignore NA’) logical expression results may not necessarily result in a NA value depending on whether the term depends onNA or not (eg TRUE || NA has the result of TRUE, but FALSE || NA has result NA) With this last one, we must be especially careful as the NA in the conditional term results in a mistake: if (NA &lt;2) print (&quot;Success!&quot;) # error !! In this lesson we have come to know the basic elements of the language R. In working with R, we usually work with complex types of data that we will learn in the following - namely, vectors, matrices, data frames and lists. Exercises What is the result of the following commands? Consider a possible result before executing. as.complex(2) as.integer(-3.25) as.logical(&quot;0&quot;) as.numeric(TRUE) as.character(11.5 + 2i) as.numeric(&quot;ABC&quot;) How do the following expressions look like in R: “three times ten on the power of nine” “logarithm of 5” “integer division of 10 by 3” “the remainder of tnteger division of 10 by 3” “tangent of 75 degrees” | Using the if expression, check whether the result of dividing whole number with NULL is considered to beNA, NaN orNULL. Enter x in the variable5. Print all environment variables. Then put NULL in the x variable. Is this variable still there? Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["vektori.html", "3 Vectors, matrices and lists 3.1 Vector 3.2 Index vectors 3.3 Matrices and arrays 3.4 Lists Exercises", " 3 Vectors, matrices and lists 3.1 Vector The vector is one of the main “complex” types of data in the language R, in the sense that it contains more values of the same type. It is similar to the term “array” in the C language. But there is one important difference here, which is necessarily adopted since it is one of the most important characteristics of the language R - in R (almost) each variable type is actually a vector. Even the variables and constants we used in the previous lesson were actually single-element vectors. This has far-reaching consequences to be discussed in detail below, and to begin with, we will first get acquainted with the syntax of creating and managing vectors. 3.1.1 Create a vector The new vector(having more than one element) is created using the c (from * combine *) function. # numeric vector m &lt;- c(1, 2, 3, 4, 5) # logic vector v &lt;- c(T, F, T) # character vector names &lt;- c(&quot;Ivo&quot;, &quot;Pero&quot;, &quot;Ana&quot;) So, simply said, the vector is arranged with a set of elements of the same type. If we create a new vector with elements of different types of data, R will automatically convert all elements into the “strongest” type, which will eventually become the type of vector itself (the term “stronger” type in this context means the type option to store all the information “stored in weaker” type, and in general the conversion goes in the direction of logic -&gt; numeric -&gt; character type). Zadatak 3.1 - creating vectors # create a new vector `x` with four arbitrary elements of the following types: # logical, real, character and integer # print the vector content and its class screen # create a new vector `x` with four arbitrary elements of the following types: # logical, realistic, character and integer x &lt;- c(T, 1.25, &quot;Ivo&quot;, 10L) # print the vector content and its class on screen x class(x) ## [1] &quot;TRUE&quot; &quot;1.25&quot; &quot;Ivo&quot; &quot;10&quot; ## [1] &quot;character&quot; The vector can be explicitly converted to another type with the help of already familiar functions as.&lt;type_name&gt;. If the conversion is impossible to implement, the element will be converted to NA with a suitable warning. Zadatak 3.2 - explicit vector type conversion x &lt;- c(1, T, 2L) y &lt;- c(1L, 2L, 3L) z &lt;- c(1.25, TRUE, &quot;Ana&quot;) # Consider a possible result and then try to make the following conversions # vector `x` in numeric type # vector `y` in character type # vector `z` in a whole type # Consider a possible result and then try to make the following conversions # vector `x` in numeric type # vector `y` in character type # vector `z` in a whole type as.numeric(x) as.character(y) as.integer(z) ## Warning: NAs introduced by coercion ## [1] 1 1 2 ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## [1] 1 NA NA Can you answer the question - why in the last example did the value TRUE not become 1L but NA instead? Try to print the z vector and notice the results of the implicit conversion you might have neglected (which converts the TRUE logical value to a string of &quot;TRUE&quot; that can no longer be ‘returned’ to the numeric value 1L). With the c function we can also connect multiple vectors to one: a &lt;- c(1, 2, 3) b &lt;- c(4, 5) c &lt;- c(6, 7, 8) # variable can be called &quot;c&quot; in spite of the function c() d &lt;- c(a, b, c) # d is now c(1, 2, 3, 4, 5, 6, 7, 8) In addition to the c function, R also offers additional convenient ways of creating new vectors: : - “range” operator, giving the range from upper to lower bound, both included seq - sequence function, similar to the range operator but with additional options rep - replicate function, repeats the provided elements provided number of times Zadatak 3.3 - vector creation helper functions # print the results of the following commands # 1: 5 # rep(c(1, 2, 3), times = 3) # rep(c(1, 2, 3), each = 3) # seq(1, 5, by = 0.5) # print the results of the following commands 1: 5 rep(c(1, 2, 3), times = 3) rep(c(1, 2, 3), each = 3) seq(1, 5, by = 0.5) ## [1] 1 2 3 4 5 ## [1] 1 2 3 1 2 3 1 2 3 ## [1] 1 1 1 2 2 2 3 3 3 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Vectors can also be created by means of functions corresponding to the names of the vector types (numeric,character, etc.) whereby as a parameter we specify the desired length of the vector. This is often done as a “preparation” of the vector for subsequent filling up of real values, ie, a kind of reservation of the place in memory. What is interesting is the fact that we can also create an “empty” vector of a certain type that is still a vector, with only the length of zero (for which, for example, with the function c we can add elements later). x &lt;- numeric(2) # vector is filled with &quot;null&quot; elements, in this case (0, 0) y &lt;- character(5) z &lt;- integer(0) # &quot;empty&quot; vector! z &lt;- c(z, 1) # add to the vector the element 1 (actually &quot;merge empty vector and element 1&quot;) Finally, to check if some vector contains a certain element we can use the operator %in%: 4 %in% seq(1, 10, 2) # returns FALSE &quot;d&quot; %in% c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) # returns TRUE Let’s see now how to access some vector elements 3.1.2 Operator [ The vector element is accessed through the index operator []`, with the help of which we can modify the vector elements: a &lt;- c(2, 4, 6) a[1] # prints a value of 2 a[2] &lt;- 5 # element on the 2nd place becomes 5 a[5] &lt;- 7 # at the 5th place is added 7, and the &quot;hole&quot; is filled with NA a ## [1] 2 ## [1] 2 5 6 NA 7 Notice a somewhat unusual fact - the first element of the vector in R has the index 1, not 0! This is an important difference compared to the indexing of elements in other programming languages. The reason for this specificity is simple - R is primarily considered as a language for data analysis, especially in tabular form, and in practice it is much easier to count rows or columns in order they appear in a data set than to do constantly “add 1”. The example above actually shows a very simplified case for retrieving vector elements and modifying them. One of the specifics of the language R is the so called vectorization, that is, the principle that R often works “more things at once” - not so much in terms of parallel execution, but in terms of declaring what we want to do. Specifically, in the case of vector indexing, we rarely retrieve or modify elements one by one, but rather we include a greater number of elements at once using the principle of vectorization and recycling. Understanding these terms is crucial for learning the R language, so we will explain them in detail below. 3.1.3 Principle of vectorization and recycling The notion of vectorization or more precisely vectorized operations and functions simply means that the operations work on multiple elements at once. If we ask R to make an operation or function over a vector of values, R will perform the function or operation over each element separately and return the vector as a result. Likewise, if we carry out the binary operation between two vectors, it will be performed over the “paired” or “aligned” elements of both vectors (suppose for now that the vectors are of the same length). Zadatak 3.4 - vectorization principle x &lt;- seq(-5, 5, 1) a &lt;- 1:3 b &lt;- 4:6 # call the abs function to calculate the absolute value # over the vector `x` and print the result # add vectors `a` and` b` with operator `+` # and print the result # multiply vectors `a` and` b` with operator `*` # and print the result # call the abs function to calculate the absolute value # over the vector `x` and print the result abs(x) cat(&quot;-----------\\n&quot;) # add vectors `a` and` b` with operator `+` # and print the result a + b cat(&quot;-----------\\n&quot;) # multiply vectors `a` and` b` with operator `*` # and print the result a * b ## [1] 5 4 3 2 1 0 1 2 3 4 5 ## ----------- ## [1] 5 7 9 ## ----------- ## [1] 4 10 18 Carefully consider the results of the previous task. If necessary, sketch the vector a andb on the paper with the vertically loaded elements and notice how to R does the “pairing” of the elements. Notice that we are not talking about “vector operations” here in a strict mathematical sense, but about aligning the elements of two vectors and performing simple operations over each of these pairs. This is especially evident in the last example where there is no “multiplication of vectors” in some of the mathematical interpretations, but rather simple multiplication of the parallel elements of the two vectors. What if the vectors are not of the same length? R in this case uses principle of recycling. The Recycle Principle states that when the vectors are uneven, the shorter vector is “recycled” as many times as needed to reach the longer length of the vector. The most common scenarios of using this principle are operations where on one side we have a vector with multiple elements and on the other a single element vector. What we should avoid is a recycling scenario where the length of a “big” vector is not a multiple of the “small” length - R will still recycle a shorter vector, only it will have to be “cut off”, which will result in an appropriate warning. Zadatak 3.5 - recycle principle a &lt;- 1:4 b &lt;- c(1, 2) c &lt;- rep (5, 3) # duplicate vector `a` and print the result # divide vector `a` with vector` b` and print the result # multiply vectors `a` and` c` and print the result a &lt;- 1:4 b &lt;- c(1, 2) c &lt;- rep (5, 3) # duplicate vector `a` and print the result 2 * a # divide vector `a` with vector` b` and print result a / b # multiply vectors `a` and` c` and print the result a * c ## Warning in a * c: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 8 ## [1] 1 1 3 2 ## [1] 5 10 15 20 Now we can finally demystify the difference between “scalar” and “vector” logic operators. Scala logical operators are intended for use with single-element vectors, they return unique values of TRUE or FALSE and are suitable for use in various conditional terms. Vector logical operators use standard R’s vectorization and recycling principles, ie, they are intended to work with logical vectors and as a result give a logical vector Zadatak 3.6 - scalar and vector logical operators a &lt;- c(T, F, F) b &lt;- c(T, T, F) # apply scalar and vector version of logical operator &quot;or&quot; # over the `a` and` b` vectors and print the result # apply scalar and vector version of logical operator &quot;or&quot; # over the `a` and` b` vectors and print the result a || b a | b ## [1] TRUE ## [1] TRUE TRUE FALSE We see that the scalar version will “use” only the first pair of logic vector elements. This means we can use it in theory in conditional logic instructions, although there is no justified reason for it, and R will in this case warnus to address the fact that we are probably using the “wrong” operator. The next example with the comparison operators may initially seem trivial, but it is important to pay special attention to the results we get since they will have a very important implications later on. So let’s take a look at what happens in the vectorization of the parallel operators. Zadatak 3.7 - vectorization of parallel operators x &lt;- 1:5 y &lt;- seq(-10, 10, 5) # print x and y # print the result of the x &gt; y command and explain the result # print the result of the x &lt; 3 command and explain the result # print x and y x y cat(&quot;-----------\\n&quot;) # print the result of the x &gt; y command and explain the result x&gt; y cat(&quot;-----------\\n&quot;) # print the result of the x &lt; 3 command and explain the result x &lt; 3 ## [1] 1 2 3 4 5 ## [1] -10 -5 0 5 10 ## ----------- ## [1] TRUE TRUE TRUE FALSE FALSE ## ----------- ## [1] TRUE TRUE FALSE FALSE FALSE Thus, by vectorizing the comparison operators over the vectors (or combinations of vectors and scalars), as a result we get logical vectors. The interpretation of these results is crucial - it actually answers the question “on what indexes is the condition fulfilled by this expression”? In other words, the results actually represent a template that describes how to filter elements of a vector. This is the basic foundation of the so-called. logical Indexing, which is one of the indexing methods that we will learn below. 3.2 Index vectors We have already learned that a vector can be retrieved through a numerical index (and we did not forget the fact that the first element has an index 1). This concept can be expanded by taking multiple elements from the vector at once. which is often referred to as “slicing”. The basic principle of choosing multiple elements at once is simple - we only need to specify the indexes of the elements we want. R offers three basic ways of crawling: integer- or location- based indexing conditional or boolean-based indexing label-based indexing Which indexing we choose depends on whether we want to access the elements depending on their location, name, or condition, and each type of indexing essentially amounts to the use of a particular vector type as a parameter for the indexing operator. Such a vector is called an “index vector” because of its role. Let’s get to know each of the types of indexing in detail. 3.2.1 Location Indexing Location Indexing is the generalization of an already familiar indexing principle where we state the ordered numbers (indices) of elements we are interested in. If we want more elements, we simply put their indices “packed” into a numeric vector. Try solving the next task by using the appropriate numeric vectors as indexing parameters. Zadatak 3.8 - location-based indexing x &lt;- 1:10 # print the first element of x # print the first three vector elements x # print the first, fifth, and seventh elements of the vector x # print the first element of x x[1] # print the first three vector elements x x[1:3] # print the first, fifth, and seventh elements of the vector x x[c(1,5,7)] ## [1] 1 ## [1] 1 2 3 ## [1] 1 5 7 Thus, the location index vector is nothing other than the ordinary numeric vector we use together with the index operator to determine which elements of another vector we want to “keep”. Let’s look at some of the features of the location index vector: Zadatak 3.9 - location indexing (2) x &lt;- 1:10 # answer the following questions with the help of an appropriate example # what does index 0 return? # what does a negative index return? # what happens if you use an index outside of vector boundaries x &lt;- 1:10 # answer the following questions with the help of an appropriate example # what does index 0 return? x[0] # what does a negative index return? x[-1] # what happens if you use an index outside of vector boundaries x[20] ## integer(0) ## [1] 2 3 4 5 6 7 8 9 10 ## [1] NA Indexing is not only used to retrieve elements. By combining the indexing operator and the assignment operator we can change the vector elements (also by the principle of “multiple elements at once”: Zadatak 3.10 - location-based indexing and assignment a &lt;- 1:10 # set all vector elements of `a` from the second to the eighth place to zero # print vector `a` b &lt;- 1:20 b [2 * 1:5] &lt;- 0 # Consider what the vector `b` looks like after the above command # print the vector `b` and explain the result a &lt;- 1:10 # set all vector elements of `a` from the second to the eighth place to zero # print vector `a` a [2:8] &lt;- 0 a b &lt;- 1:20 b [2 * 1:5] &lt;- NA # Consider what the vector `b` looks like after the above command # print the vector `b` and explain the result b ## [1] 1 0 0 0 0 0 0 0 9 10 ## [1] 1 NA 3 NA 5 NA 7 NA 9 NA 11 12 13 14 15 16 17 18 19 20 3.2.2 Conditional indexing If we carefully considered the results obtained with examples with vectorized comparison operators then we can very well grasp the way conditional indexing works. The principle is simple - for the index vector we set a logical vector of the same length as the vector whose elements we want to retrieve. The logic vector elements determine which elements are retained (the positions where the value is TRUE) and which we reject (positions where the value is FALSE). Zadatak 3.11 - conditional indexing x &lt;- 1:10 # create a logical vector `y` of length 10 with an arbitrary combination of # TRUE and FALSE values # index the vector `x` with the` y` vector, print and explain the result # print all vector elements `x` which are less or equal to 5 # use the appropriate expression as a logical index vector x &lt;- 1:10 # create a logical vector `y` of length 10 with an arbitrary combination of # TRUE and FALSE values y &lt;-c(T, T, F, T, F, F, F, T, F, T) # index the vector `x` with the` y` vector, print and explain the result x[y] # print all vector elements `x` which are less or equal to 5 # use the appropriate expression as a logical index vector x[x &lt;= 5] ## [1] 1 2 4 8 10 ## [1] 1 2 3 4 5 The last command, while simple, is one of the key principles for filtering elements in the language R. The combination of the index operator and the conditional expression represents a concise but very powerful vector filtering mechanism. Let’s try this principle in a few more examples. Zadatak 3.12 - conditional indexing y &lt;- seq (1, 100, 7) students &lt;- c(&quot;Ivo&quot;, &quot;Petra&quot;, &quot;Marijana&quot;, &quot;Ana&quot;, &quot;Tomislav&quot;, &quot;Tin&quot;) # print all even, and then all odd vector elements of `y` # print all vector elements from `students` which represent 3-letter names # (note: we use the `nchar` function to count the characters in R) y &lt;- seq (1, 100, 7) students &lt;- c(&quot;Ivo&quot;, &quot;Petra&quot;, &quot;Marijana&quot;, &quot;Ana&quot;, &quot;Tomislav&quot;, &quot;Tin&quot;) # print all even, and then all odd vector elements of `y` y[y %% 2 == 0] y[y %% 2 != 0] # print all vector elements from `students` which represent 3-letter names # (note: we use the `nchar` function to count the characters in R) students [nchar(students) == 3] ## [1] 8 22 36 50 64 78 92 ## [1] 1 15 29 43 57 71 85 99 ## [1] &quot;Ivo&quot; &quot;Ana&quot; &quot;Tin&quot; If the concept of conditional indexing with the help of conditional expressions is still unclear, one of the things that can help is to sketch the intermediate results - simply print the result of the expression within the square bracket of the index operator and then consider how that result affects the final solution. 3.2.3 Label-based indexing label-based indexing works on the principle of explicitly naming the elements we want to “keep”. In order to be able to use this type of indexing we must meet the necessary prerequisite - vector elements must have predefined “names”. The vectors we used so far did not have named elements. Each element had its predefined position within the vector and its value but did not have any special additional identifiers. Programming language R allows you to attach names to vector elements in a very simple way - using a combination of the names function, the assignment operator, and the character vector with selected names. We need to make sure that the vector name is of the same length as the original vector! Zadatak 3.13 - label-based indexing height &lt;- c(165, 173, 185, 174, 190) names (height) &lt;- c(&quot;Marica&quot;, &quot;Pero&quot;, &quot;Josip&quot;, &quot;Ivana&quot;, &quot;Stipe&quot;) # print the vector `height` # print the height of Pero and Josip height &lt;- c(165, 173, 185, 174, 190) names (height) &lt;- c(&quot;Marica&quot;, &quot;Pero&quot;, &quot;Josip&quot;, &quot;Ivana&quot;, &quot;Stipe&quot;) # print the vector `height` height # print the height of Pero and Josip height [c(&quot;Pero&quot;, &quot;Josip&quot;)] ## Marica Pero Josip Ivana Stipe ## 165 173 185 174 190 ## Pero Josip ## 173 185 We see that label-based indexing needs a corresponding character vector as the index parameter. (NOTE: A more careful reader will notice an unusual fact in the program code - function call is used as a lvalue! Answer to the question why this works may require a little more knowledge of the internal functioning of the R language and for now it is enough to say that here it’s actually calling a function called names&lt;- which is “hidden” behind a much more intuitive and easy-to-understand syntax). If for some reason we want to delete the names of vector elements, simply forward NULL to names. names &lt;- NULL This will conclude the story of the index vectors. We learned different ways of creating a vector and getting and modifying its elements. Now is the time to try to add the additional “dimension” to the vectors - let’s get to know the matrices and the arrays. 3.3 Matrices and arrays The matrices and the arrays are simply - multidimensional vectors. Matrix is a vector of two dimensions, that is, a vector that organizes elements in “rows” and “columns”. Array is a vector with three or more dimensions. While matrices are used relatively often in practice, the arrays are somewhat more limited to specific scenarios. Because of this fact in this chapter we will mainly deal with matrices, although the concepts presented are very easily applicable to arrays. What is common to the matrices and arrays, which is a well-known fact to readers with programmatic experience, is that their multidimensionality is actually virtual. Both the matrix and the array are actually one-dimensional vectors with the attribute of dimensionality, and with this attribute the language R maps our multidimensional indexing to the “real” index of the element of the one dimensional vector. This fact does not limit us - we can still in most cases treat the matrix as if it is actually two-dimensional, and knowledge of one-dimensional nature can only give us additional flexibility in working with the matrices. There are several ways to create a new matrix: With the help of the matrix function to which we forward the one-dimensional vector and the desired number of rows and columns through the nrow and ncol parameters “manually” by setting dimensions of a one dimensional vector using the dim function and associating a two-element numeric vector with matrix dimensions “binding” rows or columns together with functions rbind (row-bind) andcbind (column-bind) We will demonstrate these ways in the following examples. Zadatak 3.14 - matrix function x &lt;- 1:12 # create a matrix with 3 rows and 4 columns using the `matrix` function # print the result on the screen # repeat the procedure but add the parameter `byrow = T` to the calling function # print the result on the screen and compare it to the previous result # create a matrix with 3 lines and 4 columns using the `matrix` function # print the result on the screen matrix (x, nrow = 3, ncol = 4) # repeat the procedure but add the parameter `byrow = T` to the calling function # print the result on the screen and compare it to the previous result matrix (x, nrow = 3, ncol = 4, byrow = T) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Note that unless explicitly requested otherwise, the R matrix is filled by columns. This is done because of the similarity of the matrix with the tabular representation of the data most often analyzed by looking at individual columns. But since we often feel that filling by the rows is more “natural”, we must not forget the very useful parameter of byrow. Zadatak 3.15 - dim function m &lt;- 1:10 # print the result of call of the `dim` function to the vector` m` # use the `dim` function to vector` m` with the assigment of the vector c(2, 5) # print `m` and comment the result # print the call results of `nrow` and` ncol` on the matrix `m` m &lt;- 1:10 # print the result of call of the `dim` function to the vector` m` dim (m) # use the `dim` function to vector` m` with the assigment of the vector c(2, 5) dim (m) &lt;- c(2, 5) # print `m` and comment the result m # print the call results of `nrow` and` ncol` on the matrix `m` nrow(m) ncol(m) ## NULL ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 ## [1] 2 ## [1] 5 We see that the “ordinary” vector does not actually have a dimension, which is manifested by the NULL values we get as a result. By invoking the function dim we can add the attribute dim so it formally becomes a matrix (or array in general case). Exact dimensions are those that define how elements are organized in rows and columns, and by setting dimensions we need to be careful that they match the current number of elements. Once the matrix has dimensions added, we can retrieve them again by using the dim function, or just the number of rows or columns with the nrow and ncol functions. The resulting matrix is like the one in the previous example filled in by the columns. Since here we do not have the opportunity to use the byrow parameter, one of the ways to get a matrix filled by rows is to transpose the resulting result with the t function. m &lt;- t (m) # transpose the matrix and store it back in the variable `m` Finally, a matrix can be created by “gluing” rows and columns with the help of rbind andcbind. This is also a convenient way to add new rows and columns to an existing matrix. Zadatak 3.16 - functions ‘rbind’ and ‘cbind’ a &lt;- 1:4 b &lt;- 5:8 c &lt;- c(0.0) # create a matrix `m` in which vectors `a` and `b` will be columns # add a new row to the top of the matrix `m` with vector elements` c` # print matrix `m` a &lt;- 1:4 b &lt;- 5:8 c &lt;- c(0.0) # create a matrix `m` in which vectors` a` and `b` will be columns m &lt;- cbind (a, b) # add a new row to the top of the matrix `m` with vector elements` c` # print matrix `m` m &lt;- rbind (c, m) m ## a b ## c 0 0 ## 1 5 ## 2 6 ## 3 7 ## 4 8 3.3.1 Matrix splicing All the learned principles for “cutting” the vector using index vectors can be applied on matrices. The differences are as follows: we index each dimension individually first we index the lines, then the columns, and divide the index vectors by comma If we want “all rows” or “all columns” we simply omit this index vector( but we still use a comma) # assume that `m &#39;is a matrix of 3 x 5, with column names from `a` to `e` m[1, 2:5] # first line, all columns from second to fifth m[c(F, T, T), c(&quot;a&quot;, &quot;b&quot;)] # second and third rows, columns `a` and` b` m[,] # all rows and all columns (same as just `m`) In practice, the matrix usually uses location-based and label-based indexing; conditional indexing is not too practical because of the two-dimensional nature of the matrix (although it is feasible, we just have to keep in mind that the logic index vector lengths correspond to the corresponding dimension). One of the things we need to keep in mind is the R-language tendency to “help” us by simplifying the result. Thus, the result of a row cutting operation that leaves only one row or column will automatically become a vector, ie lose the dimension attribute. This sometimes does not suit us, especially if we are working on scripts that expect a matrix further down the line, even though it has the dimension of rows or columns 1. In this case, we need to set an additional parameter drop = F for indexing. This often seems quite unattractive, which is why there are many R-language packages that “repair” this, that is to say, try to leave the results consistent. But the drop parameter set toFALSE should be taken into account, as it will appear in other places in a similar function. Zadatak 3.17 - matrix splicing m &lt;- matrix (1:30, 6, 5, T) colnames (m) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # print all elements of the matrix m from the second to the fourth line # and from the third to the fifth column # set all elements in column &quot;c&quot; to zero # and then print the first two lines of matrix `m` # print only column &quot;d&quot; # rewrite column &quot;d&quot;, but add the `drop = FALSE` parameter when indexing # Separate the parameter with a comma (as if it was a &quot;third&quot; indexing dimension) m &lt;- matrix (1:30, 6, 5, T) colnames (m) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # print all elements of the matrix `m` from the second to the fourth line # and from the third to the fifth column m [2:4, 3:5] # set all elements in column &quot;c&quot; to zero # and then print the first two lines of matrix `m` m [, &quot;c&quot;] &lt;- 0 m [1:2] # print only column &quot;d&quot; m [, &quot;d&quot;] # rewrite column &quot;d&quot;, but add the `drop = FALSE` parameter when indexing # Separate the parameter with a comma (as if it was a &quot;third&quot; indexing dimension) m [, &quot;d&quot;, drop = F] ## c d e ## [1,] 8 9 10 ## [2,] 13 14 15 ## [3,] 18 19 20 ## [1] 1 6 ## [1] 4 9 14 19 24 29 ## d ## [1,] 4 ## [2,] 9 ## [3,] 14 ## [4,] 19 ## [5,] 24 ## [6,] 29 Here we will finish the story of the matrices. These structures are very useful in solving linear algebra tasks, so it is often convenient to look at the documentation of the R language to see which functions and operators are available to us for such a job. Similarly, some of the principles of matrix management will be useful in managing the so-called “data frames” - one of the most useful data structures in R. Finally, although we will not deal in detail with the arrays, we will show the example of the program code that creates a three-dimensional array and then uses standard cutting principles we encountered with the vectors and matrices. myArray &lt;- array(1:24, dim = c(2, 3, 4)) # array of dimension 2 x 3 x 4 myArray[, 1:2, 3, drop = FALSE] # print all rows, first and second columns # 3rd &quot;layer,&quot; with array type retention 3.4 Lists The list is an element in R used as a “universal container” of any data. Unlike the vector (ie, the concept of vector as we initially defined it), the list may contain different types of data or, more often, sets of different types of data. We create the list with the list function by which we add a string of names of elements and their content. These elements can be anything, even other lists. myList &lt;- list(a = 1, b = 2:100, c = list(x = 1, y = 2)) Try to create your own list in the following example. Zadatak 3.18 - list creation # create a new list called `stuff` that will have the following elements # element called `numbers&#39; with integers from 1 to 3 # element called `letters&#39; with letters &#39;A&#39; and &#39;B&#39; # nameless element with logical vector `c(T, F)` # element called `titles&#39; with the concent of &#39;Mr&#39;, &#39;Mrs&#39; and &#39;Ms&#39; # print the `stuff` variable stuff &lt;- list(numbers = c(1,2,3), letters = c(&quot;A&quot;, &quot;B&quot;), c(T, F), titles = c(&quot;Mr&quot;, &quot;Mrs&quot;, &quot;Ms&quot;)) # print the `stuff` variable stuff ## $numbers ## [1] 1 2 3 ## ## $letters ## [1] &quot;A&quot; &quot;B&quot; ## ## [[3]] ## [1] TRUE FALSE ## ## $titles ## [1] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; Note that the list keeps the order of elements - the element without the name is shown in index 3. The str (“structure”) function allows us to inspect the properties and list contents without printing the entire list. This function is often used by analysts, both for viewing the list and for a quick insight into the already mentioned data frames that we will work with in the next chapter. Zadatak 3.19 - list structure # print the structure of the `stuff` list # print the structure of the `stuff` list str(stuff) ## List of 4 ## $ numbers: num [1:3] 1 2 3 ## $ letters: chr [1:2] &quot;A&quot; &quot;B&quot; ## $ : logi [1:2] TRUE FALSE ## $ titles : chr [1:3] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; At the beginning of this lesson, we have said that the principle “all is a vector” is very important in R and that the vectors actually have arranged sets of elements of the same type. From this one could conclude that this fact is not valid for the lists - they obviously contain elements of different types. But the real answer is - the lists are actually vectors, and the definition is actually still consistent. That is, all the elements of the list are actually a small single-element lists, so all elements are formally of the same type. Zadatak 3.20 - list element type # print the first element of the list `stuff` # check its type # print the first element of the list `stuff` stuff[1] # check its type typeof(stuff[1]) ## $numbers ## [1] 1 2 3 ## ## [1] &quot;list&quot; So, we’ve proved that the list elements are actually a small list, as seen from printing the item itself, as well as checking its type. It may seem that the elements above created lists should be vectors, since we have created the list by “stacking” different vectors, however in the process of list creation all these elements are wrapped in single-element lists. Often we do not want to work with a list element as a “small list”, but want to have it in its “original” form. For this we use the operator [[, ie the operator of “double angular brackets”. Zadatak 3.21 - operator [[ # print the first element of the list `stuff` using the operator `[[` # check its type # print the first element of the list `stuff` using the operator `[[` stuff[[1]] # check its type typeof(stuff[[1]]) ## [1] 1 2 3 ## [1] &quot;double&quot; The aforementioned operator is most often used to retrieve the selected element of the list defined by the number or (if defined) by the element name. In this approach, we must use the syntax such as list[[name_element]] symbol, which is somewhat clumsy for typing. Because of this, R offers an alternative way of accessing the list elements by their name using the $ operator, ie list$name_element. Zadatak 3.22 - operator ‘$’ # print the `letters` element of the `stuff` list # using `[[` the operator # print the `letters` element of the `stuff` list # using the `$` operator # print the `letters` element of the `stuff` list # using `[[` the operator stuff[[&quot;letters&quot;]] # print the `letters` element of the `stuff` list # using the `$` operator stuff$letters ## [1] &quot;A&quot; &quot;B&quot; ## [1] &quot;A&quot; &quot;B&quot; The lists are an extremely popular type of object in R, as they represent a universal template for more complex data structures, including more complex objects in the narrower sense (as we will see later). The list is also the “foundation” for the most popular and most commonly used element of the R-language: the data frame - which we will learn in the next lesson. Finally, we learn how to add an element to the list. This is easiest to do using the aforementioned operator $ - such as list$newElement &lt;- newElementContents. The element is deleted by assigning the value NULL. Zadatak 3.23 - adding list elements # in the `stuff` list add the `evenNumbers` element which contains # all even numbers from 1 to 100 # delete the third element from the list # print the `stuff` list # in the `stuff` list add the `evenNumbers` element which contains # all even numbers from 1 to 100 stuff$evenNumbers &lt;- seq(2, 100, 2) # delete the third element from the list stuff[[3]] &lt;- NULL # print the `stuff` list print(stuff) ## $numbers ## [1] 1 2 3 ## ## $letters ## [1] &quot;A&quot; &quot;B&quot; ## ## $titles ## [1] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; ## ## $evenNumbers ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 ## [18] 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 ## [35] 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 In the next lesson, we will finally get to know the often-mentioned data frames as the most popular and most commonly used data structures of the language R. Exercises Create the following vectors: (11, 12, 13,…, 99) (0, 0, 0, 0, … , 0) (100 zeris) (0, 0.1, 0.2, …., 1.0) What is the sum of all numbers from 101 to 1001, if we skip all numbers divisible by 10? Use the sum function. Create a 3 x 3 matrix by performing the following commands (the sample function will be done in one of the following lessons): # we create a matrix of 3x3 randomly selected elements from 1 to 100 set.seed(1234) m &lt;- matrix(c(sample(1:100, 9, T)), nrow = 3, ncol = 3, byrow = T) Calculate the inverse matrix with the solve function. Make sure the multiplication of the original and inverse matrix result with the unit matrix (use the % *% operator to multiply the matrices). Initialize the stuff list used in the lesson. Do the following: print the class of the second element of the list print the element in the third place of the element of the list named letters check the length of the element called titles and add the title ‘Prof’ to the last position check if the number 4 is contained in the first element of the list add a new list of three vectors a,b and c which all contain elements (1,2,3) to the last place of the list, Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["okviri.html", "4 Data frames and factors 4.1 Data frames 4.2 Selecting rows and columns 4.3 Adding and deleting rows and columns 4.4 Factors Exercises", " 4 Data frames and factors 4.1 Data frames As already mentioned, the data frame is by far the most popular element of the programming language R. The language R’s primary fnction is data analysis, and the data frame is actually an object representation of the data set we intend to analyze. In other words, the data frame is an object similar in function to a sheet in Microsoft Excel or a table in a relational database. Almost every session in R is oriented around manipulating data frames - but while in Excel we manage the table with the help of a graphical interface, and in relational database via the query language SQL, in R we manage data programatically through the data frames. Take for example the following table: zipCode cityName avgSalKn population cityTax 10000 Zagreb 6359.00 790017 18 51000 Rijeka 5418.00 128384 15 21000 Split 5170.00 167121 10 31000 Osijek 4892.00 84104 13 20000 Dubrovnik 5348.00 28434 10 This data set that contains certain parameters related to cities in the Republic of Croatia (these valuesdo not necessarily correspond to the current state but are used only for demonstration). We can easily imagine how to write this data in Excel or create a relational table of names, for example, where we store the data. Let’s show now how to manipulate the same information within the R language, ie let’s try to create a data frame that will contain this data. In the last lesson we noted that the list as a complex type which serves as a kind of “template” with the help of which we can collect a number of different objects within the same structure. The data frame is in fact nothing but a list - that is, a “container” that can contain other data containers of different types. But while the “list” is actually a universal container, ie we do not have the limitations about stuff we place into it, the data frame has certain restrictions. The most important limit imposed by the data frame is that each element within the data frame must have the same number of elements. Why is that so? Let’s imagine a list in which each element has the same number of sub-elements. If each element is sketched vertically, with subelements written one below another, then these sub-elements will be “aligned” by the rows, which means that we have achieved classical data organization in columns (list elements) and rows (aligned sub-elements). Thus, this restriction directly imposes a “tabular” or “matrix” list structure with clearly defined rows and columns, which actually allows us to manage the data frame by using not only list methods, but also methods primarily intended for matrices. There are several ways to create data frames, and we’ll show two of the most frequently encountered scenarios in practice: Programatic creation via the data.frame function Load data from an external source using the read.csv function Let’s see both of these cases. First, we will create a data frame programatically. Zadatak 4.1 - creating data frames programatically zipCode = c(10000, 51000, 21000, 31000, 2000) cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;) avgSalKn = c(6359., 5418., 5170., 4892., 5348.) population = c(790017, 128384, 167121, 84104, 28434) cityTax = c(18, 15, 10, 13, 10) # create a new data frame using the `data.frame` function # name the variable `cities` # use the same syntax as when creating a list # use the names of the variables for names of list elements # print the variable `cities` zipCode = c(10000, 51000, 21000, 31000, 2000) cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;) avgSalKn = c(6359., 5418., 5170., 4892., 5348.) population = c(790017, 128384, 167121, 84104, 28434) cityTax = c(18, 15, 10, 13, 10) # create a new data frame using the `data.frame` function # name the variable `cities` # use the same syntax as when creating a list # use the names of the variables for names of list elements cities &lt;- data.frame( zipCode = zipCode, cityName = cityName, avgSalKn = avgSalKn, population = population, cityTax = cityTax) # print the variable `cities` cities ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 If you like, try creatint the upper data frame again, but with different numbers of vector element numbers that make up the columns. This operation will result in an error with a appropriate message and the data frame will not be created - R tries to keep the matrix nature of the frame always preserved. A little note regarding terminology: the “data frame” is often referred to as simply a “frame” or a “table”. Likewise, we often use the “column”, “variable” or “attribute” terms for the vertical data frame elements, while referring to the horizontal frame elements as “rows” or “observations”. These terms are in line with the standard way of referencing the table elements and the statistical terms referring to the datasets. If there is a chance of ambiguity from the context, the term that clearly describes the referenced element will be used. Let’s try to load the table from an external source now. Although R allows different forms of external data, we will assume that the data is obtained in a standard “CSV form” (CSV - comma-separated values). This form is one of the most popular data storage methods in a pure text format that has the advantage of being easy to create manually, and most data management tools implement logic to import/export data as a CSV file. Below we can see an example of a CSV file that matches the data frame created in the previous example. Suppose the file is named cities.csv. The data is separated by a comma (no spaces!), Every observation in your line, and the optional first line represents the column names. zipCode,cityName,avgSalKn,population,cityTax 10000,Zagreb,6359.00,790017,18 51000,Rijeka,5418.00,128384,15 21000,Split,5170.00,167121,10 31000,Osijek,4892.00,84104,13 20000,Dubrovnik,5348.00,28434,10 One of the potential problems with CSV files is that they use the comma as separator (delimiter) of column elements, and in certain languages, as a standard, a “decimal comma” is used instead of a decimal point. Because of this fact, there is an “alternative” CSV standard that uses a semi-colon as a separator so that our CSV file in this case looks like this (let’s call it citiesAlt.csv): zipCode;cityName;avgSalKn;population;cityTax 10000;Zagreb;6359,00;790017;18 51000;Rijeka;5418,00;128384;15 21000;Split;5170,00;167121;10 31000;Osijek;4892,00;84104;13 20000;Dubrovnik;5348,00;28434;10 Since the decimal point is a standard in the Republic of Croatia, in working with CSV files we have to be careful which of the two standard records is used. Luckily, the R language offers support functions for both standards, so we do not have to adapt the input files specifically, just be careful what function we will choose. Suppose we have these two files in the workbook: cities.csv citiesAlt.csv (If we do not have these files available, we can easily create them with the help of plain text editors (eg Notepad or gedit) and copying the above rows.) To create data frames from CSV files we use the following functions: - read.csv - for “normal” CSV files with a comma as separator - read.csv2 - for an alternative CSV standard that use semi-colons The main input parameter of these functions is the path to the CSV file that we load. Functions also have a rich set of additional parameters that allow them to be adapted to different scenarios, and if we get some of the “exotic” forms of CSV files, it is worth looking at the read.table function which is very flexible with respect to the number of different parameters and data load settings. Functions read.csv andread.csv2 are actually derived from the read.table function by defaulting certain parameters to standard CSV features. Some of the parameters and associated values of read.csv (or read.table) functions that are useful to know are: header = FALSE - for files without a header sep = &quot;#&quot; - for files that use some more “exotic” separator, in this case # na.strings = 'NULL' - the term used in the dataset to represent the missing values that will become NA in R nrows = 2000 - maximum number of lines to be read, in this case 2000 stringsAsFactors = F - preventing automatic creation of factor columns (which we will learn in the lesson below) encoding = 'UTF-8' - for non-ASCII text encoding standards (especially if we are working with Croatian speaking area data using diacritical characters) Let’s try to load data from available CSV files now. This data will not require special parameters, and will only be loaded by providing the path to the associated files (one that uses the comma and the other that uses the semi-colon as a separator). Zadatak 4.2 - reading CSV files # load data from files `cities.csv` and` citiesAlt.csv` # save data in variables called `cities2` and` cities3` # print `cities2` and` cities3` # load data from files `cities.csv` and` citiesAlt.csv` # save data in variables called `cities2` and` cities3` cities2 &lt;- read.csv(&quot;cities.csv&quot;) cities3 &lt;- read.csv2(&quot;citiesAlt.csv&quot;) # print `cities2` and` cities3` cities2 cat(&quot;-----------\\n&quot;) cities3 ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 20000 Dubrovnik 5348 28434 10 ## ----------- ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 20000 Dubrovnik 5348 28434 10 Let’s look at some useful functions for working with data frames, ie tables. A good part of them is already known from experience in working with lists and matrices: nrow - number of lines ncol orlength - the number of columns (since the frame behaves both as a matrix and as a list) dim - table dimensions names - column names head - prints several rows from the beginning of the table tail - prints several rows from the end of the table str - prints table structure summary - summarizes the statistical information about table columns Let’s try some of these functions: Zadatak 4.3 - working with data frames # print the dimensions of the data frame `cities` # print the table structure of `cities` # print the first few rows of the data frame `cities` # print summarized statistical information about `cities` # print the dimensions of the data frame `cities` dim(cities) cat(&quot;-----------\\n&quot;) # print the table structure of `cities` str(cities) cat(&quot;-----------\\n&quot;) # print the first few rows of the data frame `cities` head(cities) cat(&quot;-----------\\n&quot;) # print summarized statistical information about `cities` summary(cities) ## [1] 5 5 ## ----------- ## &#39;data.frame&#39;: 5 obs. of 5 variables: ## $ zipCode : num 10000 51000 21000 31000 2000 ## $ cityName : Factor w/ 5 levels &quot;Dubrovnik&quot;,&quot;Osijek&quot;,..: 5 3 4 2 1 ## $ avgSalKn : num 6359 5418 5170 4892 5348 ## $ population: num 790017 128384 167121 84104 28434 ## $ cityTax : num 18 15 10 13 10 ## ----------- ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ----------- ## zipCode cityName avgSalKn population ## Min. : 2000 Dubrovnik:1 Min. :4892 Min. : 28434 ## 1st Qu.:10000 Osijek :1 1st Qu.:5170 1st Qu.: 84104 ## Median :21000 Rijeka :1 Median :5348 Median :128384 ## Mean :23000 Split :1 Mean :5437 Mean :239612 ## 3rd Qu.:31000 Zagreb :1 3rd Qu.:5418 3rd Qu.:167121 ## Max. :51000 Max. :6359 Max. :790017 ## cityTax ## Min. :10.0 ## 1st Qu.:10.0 ## Median :13.0 ## Mean :13.2 ## 3rd Qu.:15.0 ## Max. :18.0 4.2 Selecting rows and columns We’ve already said that data frames behave both as matrices and as lists, which is a feature we often use when selecting rows and columns of data frames. Specifically, we use the following: for selecting rows and columns: two-dimensional indexing with the help of index vectors for select a column: operator $ Here we are actually quite flexible - we can, for example, first get certain matrix rows with the help of location indexing and then only single column with the help of $. In practice, one of the most common combinations is the conditional selection of rows with the named selection of columns (SQL experts will recognize this as a standard combination of WHERE and SELECT). Let’s try to apply our knowledge of index vectors, matrices, and lists. Zadatak 4.4 - selecting rows and columns # print the table `cities` (for reference) # print the first three rows, the third and fifth column # print column &quot;cityTax&quot; # print zipCodes and city names of all cities which # have cityTax greater than 12% and a population of more than 100,000 # print the table `cities` (for reference) cities cat(&quot;-----------\\n&quot;) # print the first three rows, the third and fifth column cities[1:3, c(2,5)] cat(&quot;-----------\\n&quot;) # print column &quot;cityTax&quot; cities$cityTax cat(&quot;-----------\\n&quot;) # print zipCodes and city names of all cities which # have cityTax greater than 12% and a population of more than 100,000 cities[cities$cityTax &gt; 12 &amp; cities$population &gt; 100000, c(&quot;zipCode&quot;, &quot;cityName&quot;)] ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ----------- ## cityName cityTax ## 1 Zagreb 18 ## 2 Rijeka 15 ## 3 Split 10 ## ----------- ## [1] 18 15 10 13 10 ## ----------- ## zipCode cityName ## 1 10000 Zagreb ## 2 51000 Rijeka Notice the similarity between the last expression and SQL query: SELECT zipCode, cityName FROM cities WHERE cities.cityTax &gt; 12 AND cities.population &gt; 100000 Choosing columns and rows is not difficult if we are well acquainted with index vector knowledge, but as it can be seen in the last example of syntax, it is often not too readable (as compared to, for example, the SQL syntax that performs the same job). For this reason, there are different R extensions that greatly facilitate this work, which we will elaborate in detail in one of the future lessons. 4.3 Adding and deleting rows and columns To add and delete rows and columns, remember that the data frame is a kind of hybrid of a matrix and a list, ie if we know how to add rows and columns to a matrix or add new elements in a list, then we can add data to the data frame in an equivalent way. When working with data frames, adding new columns is a bit more common than adding rows. As we have said, we add columns to the data frame in the same way as we add list elements - while paying attention that the added column has the same number of elements as the other columns. New columns are often derived from existing columns to create new binary indicators, the results of arithmetic data expressions using other columns, etc. Zadatak 4.5 - adding new columns to a data frame # add the logical column` highCityTax` to the `cities` table # which will indicate whether the cityTax is greater than 12% # assume the following (imaginary) way of calculating city tax income # - cities have about 60% of working population # - each worker pays a tax that is roughly equal to 10% of net salary # - income from city tax per worker is (cityTaxa percentage)*(tax income) # # add a `monthlyTaxIncome` column which will use the average salary, city tax # rate and population to estimate hte monthly tax income of the cities # (in million Kns) # i broja stanovnika procijeniti koliki prihod pojedino cities dobija od cityTaxa # round up to two decimals ( using the `round` function, # .e.g. : round(100.12345, 2) ==&gt; 100.12 ) # print `cities` # add the logical column` highCityTax` to the `cities` table # which will indicate whether the cityTax is greater than 12% cities$highCityTax &lt;- cities$cityTax &gt; 12 # assume the following (imaginary) way of calculating city tax income # - cities have about 60% of working population # - each worker pays a tax that is roughly equal to 10% of net salary # - income from city tax per worker is (cityTaxa percentage)*(tax income) # # add a `monthlyTaxIncome` column which will use the average salary, city tax # rate and population to estimate hte monthly tax income of the cities # (in million Kns) # i broja stanovnika procijeniti koliki prihod pojedino cities dobija od cityTaxa # round up to two decimals ( using the `round` function, # .e.g. : round(100.12345, 2) ==&gt; 100.12 ) cities$monthlyTaxIncome&lt;- round(0.6 * cities$population * 0.01 * cities$avgSalKn * 0.01 * cities$cityTax / 1e6 , 2) # print `cities` cities ## zipCode cityName avgSalKn population cityTax highCityTax ## 1 10000 Zagreb 6359 790017 18 TRUE ## 2 51000 Rijeka 5418 128384 15 TRUE ## 3 21000 Split 5170 167121 10 FALSE ## 4 31000 Osijek 4892 84104 13 TRUE ## 5 2000 Dubrovnik 5348 28434 10 FALSE ## monthlyTaxIncome ## 1 5.43 ## 2 0.63 ## 3 0.52 ## 4 0.32 ## 5 0.09 You can also add rows and columns similar to adding rows and columns to a matrix - with the rbind andcbind functions. For rbind we usually add a new data frame with rows that have the appropriate order and type of elements, while we can add a completely new vector with cbind, keeping in mind that the number of elements corresponds to the number of rows of the original data frame. Let’s try these functions on small “artificial” data frames to better demonstrate their functionality. Zadatak 4.6 - data frames and rbind/cbind functions df1 &lt;- data.frame(a = c(1,2,3), b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c = c(T, F, T)) df2 &lt;- data.frame(a = 1, b = &quot;A&quot;, c = 3) # make a data frame which has `df1` and `df2` as rows # name it `df12` # add a `firstNames` columns containing names Ivo, Ana, Pero and Stipe # use `cbind` # print `df12` df1 &lt;- data.frame(a = c(1,2,3), b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c = c(T, F, T)) df2 &lt;- data.frame(a = 1, b = &quot;A&quot;, c = 3) # make a data frame which has `df1` and `df2` as rows # name it `df12` df12 &lt;- rbind(df1, df2) # add a `firstNames` columns containing names Ivo, Ana, Pero and Stipe # use `cbind` df12 &lt;- cbind(df12, firstNames = c(&quot;Ivo&quot;, &quot;Ana&quot;, &quot;Pero&quot;, &quot;Stipe&quot;)) # print `df12` df12 ## a b c firstNames ## 1 1 A 1 Ivo ## 2 2 B 0 Ana ## 3 3 C 1 Pero ## 4 1 A 3 Stipe For deleting rows and columns we can also use the same methods for managing the matrices and lists. Specifically: deleting rows and columns can be done by using two-dimensional indexing of rows and columns that we want to “keep” deleting columns can be done by assigning the value NULL to the selected column Let’s try this in the example. *** Zadatak 4.7 - deleting rows and columns # delete the first row and second column from `df12` # use the indexing method # delete the `firstNames` column by using the `NULL` method # print `df12` # delete the first row and second column from `df12` # use the indexing method df12 &lt;- df12[-1, -4] # delete the `firstNames` column by using the `NULL` method df12$firstNames &lt;- NULL # print `df12` df12 ## a b c ## 2 2 B 0 ## 3 3 C 1 ## 4 1 A 3 Data frames will continue to be dealt with in the chapter on data management, where we will learn how to work with frameworks with far more data than the examples we have used in this lesson, and how to work with additional packages that significantly facilitate frequently used data actions in data frameworks. Below we will deal with another new (and slightly controversial) data structure. 4.4 Factors Factors in R are actually the type of data that represents what is referred to in statistics as a nominal or categorical variable. Namely, the attribute of some observation often takes on a value from a set of previously known categories (eg gender, age category, education, city of birth, political party preference, etc.). Categories are often identified by a unique set of characters, and in the process of analyzing them we often spend various aggregations and groupings (for example, in a sports race we can calculate the average time depending on gender or age category) or we share a set of data depending on the category. Factors in R are often the subject of discussion since it is a construct that can facilitate data processing but also cause problems, especially if we are not aware that at some point we are working with a factor (this scenario is actually easily avoided, as we will explain at the end of this chapter). To begin with, we will show you what factors are, with the help of a simple example. Let’s imagine that the next character vector describes the blood pressure level in ten patients: bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) This is obviously a “categorical” variable since it can take one of three discrete values - low, normal and high. Thus, this vector is a typical candidate for “factorizing”, ie for converting to a factor class object. We create these vectors by using the factor function to which we forward the character vector of category names as a parameter. Zadatak 4.8 - factorizing a character vector bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) # print the variable `bloodPressure` # print its class # create a variable `bloodPressure.f` by factorizing # the variable `bloodPressure` # print the variable `bloodPressure.f` # print its class bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) # print the variable `bloodPressure` bloodPressure # print its class class(bloodPressure) # create a variable `bloodPressure.f` by factorizing # the variable `bloodPressure` bloodPressure.f &lt;- factor(bloodPressure) cat(&quot;-----------\\n&quot;) # print the variable `bloodPressure.f` bloodPressure.f # print its class class(bloodPressure.f) ## [1] &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;normal&quot; &quot;normal&quot; &quot;low&quot; &quot;high&quot; ## [8] &quot;low&quot; &quot;normal&quot; &quot;normal&quot; ## [1] &quot;character&quot; ## ----------- ## [1] low high high normal normal low high low normal normal ## Levels: high low normal ## [1] &quot;factor&quot; We see that the factor print was given an additional attribute Levels. This means that its actually a categorical variable with lsited exact categories it is allowed to take. If we try to add a new value to a factor that is not present in the current categories (e.g., “very high”) we get a warning, and instead of the category we have specified, a new item will have the value of NA. This is sometimes not the scenario we want. If we know in advance that the character vector we categorize does not contain all the possible categories that may generally appear, we have the option of adding the levels parameter in which we will explicitly specify a series of&quot; possible &quot;categories with the help of the vector vector. Zadatak 4.9 - using the levels attribute # add an 11th element to `bloodPressure.f` called &quot;very low&quot; # print the variable `bloodPressure.f` # create a variable `bloodPressure.f2` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # add an 11th element to `bloodPressure.f2` called &quot;very low&quot; # print the variable `bloodPressure.f2` # add an 11th element to `bloodPressure.f` called &quot;very low&quot; bloodPressure.f[11] &lt;- &quot;very low&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 11, value = &quot;very low&quot;): invalid factor ## level, NA generated # print the variable `bloodPressure.f` bloodPressure.f cat(&quot;-----------\\n&quot;) # create a variable `bloodPressure.f2` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; bloodPressure.f2 &lt;- factor(bloodPressure, levels = c(&quot;very low&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;high&quot;, &quot;very high&quot;)) # add an 11th element to `bloodPressure.f2` called &quot;very low&quot; bloodPressure.f2[11] &lt;- &quot;very low&quot; # print the variable `bloodPressure.f2` bloodPressure.f2 ## [1] low high high normal normal low high low normal normal ## [11] &lt;NA&gt; ## Levels: high low normal ## ----------- ## [1] low high high normal normal low high ## [8] low normal normal very low ## Levels: very low low normal high very high What is the advantage of using factors? Why not leave the variables in the original, “character” form? The reason for the factoring of categorical columns, ie variables, is that certain statistical and visualization functions “know” how to correctly interpret and use factors and treat them differently from ordinary “character” columns. For this reason, it is a very good long-term strategy that, at the beginning of R’s learning, we are used to factorizing columns that are really categorical variables (but we also note that we must not factorize columns that are not categorical variables, which can happen if we are not careful). One of the frequently asked questions about the category variables is - how are individual categories represented? The answer to this question is provided by the table function to which we pass the selected factor. Zadatak 4.10 - the table function # print how are the categories represented in `bloodPressure.f2` # print how are the categories represented in `bloodPressure.f2` table(bloodPressure.f2) ## bloodPressure.f2 ## very low low normal high very high ## 1 3 4 3 0 The table function does not necessarily require a factor and will work even with the character vector. But in that case, we would not get information about categories that were not represented at all. The categorical variable in our examples actually has the so-called ordinal nature. Ordinal category variables means that the categories have a natural order (“low” blood pressure is lower than “normal” which in turn is lower than “high”). If desired, this fact can be “embedded” in the initialization factor by simply adding the order parameter set to TRUE. The advantage of the ordinal factor is that it allows us to compare factor values with the help of comparative operators. Zadatak 4.11 - ordinal factor # create a variable `bloodPressure.f3` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # also set the `order` paramater to `TRUE` # watch out for category ordering! # print the variable `bloodPressure.f3` # check if it is in fact the ordinal factor now # use the `is.ordered` function # check if the first patient has lower blood pressure # than the third # create a variable `bloodPressure.f3` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # also set the `order` paramater to `TRUE` # watch out for category ordering! bloodPressure.f3 &lt;- factor(bloodPressure, levels = c(&quot;very low&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;high&quot;, &quot;very high&quot;), order = TRUE) # print the variable `bloodPressure.f3` bloodPressure.f3 # check if it is in fact the ordinal factor now # use the `is.ordered` function is.ordered(bloodPressure.f3) # check if the first patient has lower blood pressure # than the third bloodPressure.f3[1] &lt; bloodPressure.f3[3] ## [1] low high high normal normal low high low normal normal ## Levels: very low &lt; low &lt; normal &lt; high &lt; very high ## [1] TRUE ## [1] TRUE We have already seen that in R “everything is a vector” - numbers are one-dimensional numeric vectors, matrices are vectors with added dimension parameter, lists are vectors of small lists, data frames are lists with added restrictions. We may wonder - what are factors? The implementation of a factor is actually a coded or enumerated set of values of initially defined character sequences, along with their associated code table. Simplifiedly, vector factorization includes: a “listing” of all detected categories (or ones set with the levels parameter) assigning numeric values in the order of each category (eg: “low” -&gt; 1, “normal” -&gt; 2 etc.) “packing” together a newly created numeric vector and the associated “code table” Although these steps R works out automatically, the internal factor structure can be determined by converting the factor into a pure numeric or pure character type. Zadatak 4.12 - internal structure of a factor # print `bloodPressure.f3` converted to character # print `bloodPressure.f3` converted to numeric # print `bloodPressure.f3` converted to character as.character(bloodPressure.f3) # print `bloodPressure.f3` converted to numeric as.numeric(bloodPressure.f3) ## [1] &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;normal&quot; &quot;normal&quot; &quot;low&quot; &quot;high&quot; ## [8] &quot;low&quot; &quot;normal&quot; &quot;normal&quot; ## [1] 2 4 4 3 3 2 4 2 3 3 By converting the factor to the character type we actually do the operation of inverse factorization, ie, we get the original character vector. On the other hand, by converting the factor into a numerical type, we actually get “pre-coded” numbers that the internal factor is used to represent each category. We may wonder - why would this be useful to us? The answer is - no, this is in practice usually has no direct benefit. Butthis is something which can help us understand a potential big problems which can catch us unaware. The above-mentioned “controversy” around the factor as a data type is actually contained in the numerical conversion of factors. It is not actually related to the factors themselves, but rather to some of the default R settings of R when it comes to loading data. Let’s look at the detailed steps that happen during the creation of the data frame from a CSV file and try to figure out where a potential problem arises: R opens a CSV file Based on the read data R tries to determine column types all character columns are automatically factorized unless the stringsAsFactors = FALSE parameter is specified The final data frame is formed Do you see a possible problem? Suppose the following scenario: in a single numeric column, a non-numeric value has somehow ocurred (e.g. a NULL character string due to the missing value in the database). This can be just one value in more than one million rows, but that’s enough for R to classify the column as a “character column”. Since R automatically factorizes the character columns, the numeric column in the final data frame becomes categorical (although the category names are actually numbers). A clumsy analyst (or an automatic script) does not notice that it is a factor, and performs the conversion of said column into a numeric type - and obtains a completely semantically meaningless set of “transcoded” integers that, if not noticed, can be used as input data for further analysis. How to avoid this scenario? The procedure is actually very simple: When using the read.csv orread.table functions, always use the string stringsAsFactors = FALSE carefully check column type data of loaded data frame perform appropriate column conversions check the results again If we adhere to these steps we will almost never come to the situation which causes us real problems. There are alternate procedures (for example, explicitly telling column types read from a file with the help of colClasses parameters, or using the as.numeric (as.character ()) syntax for converting columns to a numeric type that will perform the correct conversion independently of whether it is a character column or a “concealed” factor), but the above steps should in most cases be completely sufficient. The most important thing to remember is the key role of the parameter setting stringsAsFactors = FALSE (which is sometimes described as in literature as “mandatory”, often without any further explanation). Exercises In the folder along with this notebook, locate the file citiesNOHEADER.csv which represents the file that is the same as cities.csv except for the following features: column names are missing the gap is used as a separator Try using the documentation to load the data from this file into the variable ’citiesNH` which will be the same variable as the cities used in the lesson. In the folder along with this notebook, locate the file receipt.csv and load it into the receipt variable. Make sure that character sequences are not automatically converted into factors. Print to the screen: the number of rows in this table the number of columns in the table column names For the receipt table, do the following: factorize the itemCategory column print the code, name and price of all items of the category “sweets and snacks” cheaper than 12 Kn print how many products each category has in the receipt add a total column that will contain the total price of eacg bought item set using the price and quantity calculate the total amount of the account In the folder along with this note find the file citiesNULL.csv which, if loaded without thestringsAsFactors = FALSE parameter, can result in a problematic scenario described in the lesson. Try to do the following: load the data from this file into the variable citiesNULL where you deliberately omit the stringstringsAsFactors = F add avgSal1 column that will represent the result of using the as.numeric function over the avgSalKn column add avgSal2 column that will represent the result of using the as.character / as.numeric functions over the avgSalKn column (be careful of the order) print out citiesNULL and comment the results Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["control.html", "5 Flow control and objects 5.1 Flow control commands 5.2 Program Loops 5.3 Object models in R language Exercises", " 5 Flow control and objects 5.1 Flow control commands Under flow control commands we mostly refer to constructs for conditional execution and so-called “program loops” where the segment of the program is continually executed until (optionally) certain conditions are fulfulled. 5.1.1 Conditional execution of commands For conditional execution we use the so-called if-then block: if (condition) {block} else {block} This command is pretty straightforward. In the next exercise we will show how to avoid a relatively common error when writing if commands. Try to spot and correct it. Zadatak 5.1 - if command # execute the next conditional execution command if (2 &gt; 1) print (&quot;Success!&quot;) # find the error in the next `if-else` command and correct it if (1 &gt; 2) print (&quot;Success!&quot;) else print (&quot;Fail!&quot;) # execute the next conditional execution command if (2 &gt; 1) print (&quot;Success!&quot;) # find the error in the next `if-else` command and correct it if (1 &gt; 2) {print (&quot;Success!&quot;) } else print (&quot;Failed!&quot;) ## [1] &quot;Success!&quot; ## [1] &quot;Failed!&quot; The error occurs because R is an interpreting language that normally executes a row per line, unless we forward an “incomplete” command to R, whereby it will wait for the “rest” before it starts executing. In the previous example, the second ifcommand is actually completed in the first line, so R is “surprised” when the next line starts with else. In order to prevent this scenario, it is sufficient to explain in some way to R that the command has not yet been completed, which is most easily accomplished by opening the block in the first line and closing it in the line with else. The readers who are programming in C or Java will recognize the so-called “ternary operator” that actually represents a compact version of the if-else block: x = (a &lt;b) ? c: d # not R language! The role of this operator in R is performed by the ifelse function. Zadatak 5.2 - ifelse function a &lt;- 1:3 b &lt;- c(0, 2, 4) # what does the vector `x` look like after executing the following command? # Think about the answer and then check the correctness of the solution x &lt;- ifelse (a &lt; b, 2, 5) # what does the vector `x` look like after executing the following command? # Think about the answer and then check the correctness of the solution x &lt;- ifelse (a &lt;b, 2, 5) x ## [1] 2 2 2 2 Note that the ifelse function is vectorized, which is why it is particularly suitable for creating new columns of data frames derived from certain conditions related to the existing columns. 5.2 Program Loops In the programming language R we have three types of loops: repeat - infinite loop while - loop with checking conditions at the beginning for - iterator loop (“loop with known number of repetitions”) 5.2.1 Loop repeat repeat is the simplest loop. It has the following syntax: repeat {block} It is an “infinite” loop where, once the block is completed, it is re-executed again so on. The only way to exit this loop is to use the break command. In addition to this we have a next command that will skip the rest of the block, but it will not exit loop. Let’s see how this loop works in the next exercise. Zadatak 5.3 - repeat loop # answer the following questions before running the next block: # - will the loop run indefinitely? # - what will be printed on the screen? i &lt;- 1 repeat { i &lt;- i + 1 if (i %% 2 == 0) next print (i) if (i&gt; 10) break } ## [1] 3 ## [1] 5 ## [1] 7 ## [1] 9 ## [1] 11 We often know the exact condition for exiting the loop and want to place it in a clearly visible place so it is not “hidden” in the body of the loop. Hence, we may find the while loop to be more useful. 5.2.2 while loop while represents the “purest” form of the program loop whose syntax literally means “while the condition is met, repeat the specified code”: while (condition) {block} Zadatak 5.4 - while loop # add the loop condition so it loops # exactly 7 times i &lt;- 1 while () { print(i) i &lt;- i + 1 } # add the loop requirement so that it does # exactly 7 times i &lt;- 1 while (i &lt;= 7) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 With this loop we have to make sure that in a certain iteration conditions for exiting will be met, otherwise it also becomes an “infinite” loop. We are also free to use the next andbreak commands, which have the same function as in the repeat loop. 5.2.3 for loop A for loop or “iterator loop” serves to easily “walk” over a data structure (most commonly vector), getting element by element and doing something with it. It uses the for keyword, the name of the new iterator variable, the keywordin, and the vector whose values are taken one by one and used within the loop (note that the specified in is not the same as the %in% operator which checks if some element is contained in the vector!). The syntax of this loop is as follows: for (i in v) {do something with i} Note that here the variable i is not a “counter” - in every loop iteration it becomes the value of the element we come across. If we want to iterate by the indexes, not by the elements themselves, then we can use the construct for (i in 1:length (a)). Zadatak 5.5 - for loop a &lt;- seq(-10, 10, 4) # print vector elements of `a` one by one # with the help of `for` loop # access the elements directly # do the same, but iterate by indexes a &lt;- seq(-10, 10, 4) # print vector elements of `a` one by one # with the help of `for` loop # access the elements directly for (i in a) print(i) # do the same, but iterate by indexes for (i in 1:length (a)) print(a[i]) ## [1] -10 ## [1] -6 ## [1] -2 ## [1] 2 ## [1] 6 ## [1] 10 ## [1] -10 ## [1] -6 ## [1] -2 ## [1] 2 ## [1] 6 ## [1] 10 Notice that the second way is better if you want to change the vector elements or need information on where the element is located in the original vector. Now that now that we have learned the loop syntax, it is important to emphasize one fact - in programming language R, it is generally not recommended to use program loops. Although this may initially seem unexpected and somewhat shocking, the reason is simple - R language is designed to work by the declarative “all at once” principle. We have already seen that the principle of vectorization and recycling effectively perform jobs that would require a loop in other programming languages, and in the chapters that follow, we will see that R also offers many other constructs that avoid explicit code repetition with the requirement of a declarative syntax that automatically performs it. For example, the following example is syntactically correct: # example of unnecessary loop usage a &lt;- 1: 5 b &lt;- 6:10 c &lt;- numeric() for (i in 1:length (a)) c[i] &lt;- a[i] + b[i] but probably works slower and is much less readable than: # R syntax a &lt;- 1:5 b &lt;- 6:10 c &lt;- a + b All of this does not mean that we should not use loops in R, but that their use should be accompanied by an additional consideration of whether the loop is really needed and whether there is an alternative syntax that completes the same task but can be written declaratively (and is potentially faster, since many routines in R are implemented in language C). Early adoption of the “R” mode of thinking will result in long-term benefits that will be reflected in a more compact, cleaner, and often more effective program code. 5.3 Object models in R language R is designed as an object-oriented language, together with the mechanisms that the object-oriented paradigm requires - encapsulation (enveloping various attributes into a common entity), polymorphism (using the same function over different objects results in different operations depending on the nature of the object) and inheritance (creating new objects from the existing ones by expanding them with additional elements). R has taken its initial way of object modeling from the S language and therefore such objects are known as “S3 objects” (according to the S language version from which they were originally taken). This object model, which we will learn very soon, is actually very unconventional and simple but also as such fairly suitable for using in R, it being primarily a domain-oriented language. With the increasing number of programmersin the R community, the pressure to bring support to objects that will be more similar to the way they work in the other programming languages has increased, in order to bring better robustness in design and management of objects. All this has led to the fact that today we have formally four types of objects in the programming language R: “base”&quot; classes - basic R elements (functions, vectors, data frames) S3 objects - principle of object design taken from S language (version 3) S4 objects - a more formal and rigorous way of creating objects approaching standard object-oriented mechanisms from other languages RC objects (reference classes) - the most recent way of creating objects (introduced in version 2.12) that replicates the “classical” object-oriented principles based on the exchange of messages The existence of three different models of defining objects (we can ignore the basic since we can not formally expand it with new objects) can seem disheartening - is it necessary to learn all three models? How to distinguish them? Which one to choose? However, despite the fact that the story about the object nature of R during its development is (unnecessarily) complicated, the good news is that for most needs, it’s quite enough to learn how the S3 model works, which is also the simplest. A large number of popular R packages use only S3 classes and it is possible to work in language R for a very long time without meeting the need for learning S4 or RC models. For this reason, we will focus on the S3 objects only (readers who want more information on other models can look at a very good book called Advanced R, by the author Hadley Wickham, which deals, among other things, with the object models in R). 5.3.1 Overview of the S3 object model As already mentioned, S3 objects are actually taken from S programming language and represent a relatively primitive implementation of the concept of “object”, as far as expectations go regarding standard methods of object creation. S3 object is actually just a list with the additional class attribute. # we are creating a new object of a class `Person` pero &lt;- list(oib = &quot;12345678&quot;, lastname = &quot;Peric&quot;, weight = 78) class(pero) &lt;- &quot;Person&quot; And that’s it! Notice that we do not have a formally defined “template” of the class that we instance in the object as is the established practice in other programming languages. For S3 objects, we simply create a list and then declare that that list is an object of a certain class, although the structure of that class is only implied by the object’s appearance (and does not have to match the structure of another object declared to belong to the same class). Of course, such a convenient way of constructing objects is not advisable so we recommend that we in practice do not construct the class “manually”, but rather to write and use a special constructor function whose parameters will actually define the object’s appearance (this will be learned in the lesson about creating user-defined functions) What about inheritance, where class-child inherits properties from its parent? R enables inheritance, but also in a very informal and relatively trivial way. Rather than just listing a single “class” name with the class attribute, we create a character vector where the first element will be the class name, and the other elements will be parent classes, sorted by ‘importance’. For example, if we created a new mate object of class Employee which inherits the class Person, then it is enough to do the following: mate &lt;- list(oib = &quot;12345678&quot;, lastname = &quot;Peric&quot;, weight = 78, yearEmpl = 2001) class(mate) &lt;- c (&quot;Employee&quot;, &quot;Person&quot;) We note that all the attribute inheritance work must be done “manually”, that is, we have to make sure that the parent has the attributes of the Person class. 5.3.2 Generic functions Looking at the above way of the object design, it is legitimate to ask an additional question - where are the methods? As we know, standard object-oriented principles assume the encapsulation of the attribute but also the methods in the form of an object. Here is the basic difference between the S3 object and the “standard” objects from the other programming languages - in S3 object model the method is defined outside of the object, in the form of so-called generic functions. Why is that so? The idea is the following: when working with objects, the user (programmer, analyst) often calls the same functions (eg “print”, “draw”, “summary description”) over objects of different types. The function of the same name will work differently depending on the object over which it is called - hence the name generic function. So, the print function always results in some kind of print, but how the printing actually works depends on the object we are printing. This way of designing an object can be extremely unconventional, but the fact is that the call of a generic function makes it look far more intuitive, especially for users who do not have a big experience in programming. Specifically, let’s compare the command: start(car, speed = 20) with the command: car.start(speed = 20) By reading the first command, the car is perceived as an “object” (in terms of word service in a sentence), that is, something we do “to” this object. The second command sets the car as a subject, which is common practice in object-oriented languages, but is not in line with the general understanding of doing things to some objects. In working with programming language R we often do “similar” tasks over different objects - we print their contents, draw them on a graph, look for some concise details of them, etc. For this reason, and the fact that R is often interactive, R is designed in a way that we think what we want to do instead of asking ourselves where the function we want to call is. If we want to print an object, it is logical to just forward it to the print function ,if we want to draw it the plot function, ans if we want to summarize it - the summary function. How does a single function “know” what to do with an object? The answer is simple - the generic function is just “interface” to the “right” function we call, and the logic to find the right function is trivial - if the generic name of the function is genFun, and the name of the object class is className, the function actually being called is genFun.className. If no such function exists, the function genFun.default is called. This can easily be seen in the next exercise. Zadatak 5.6 - generic functions # print the `summary` function (only the function name!) # print a function that is actually being called when you call # the `summary` function of the `factor` class object # print the `summary` function (only the function name!) summary # print a function that is actually being called when you call # the `summary` function of the `factor` class object summary.factor ## standardGeneric for &quot;summary&quot; defined from package &quot;base&quot; ## ## function (object, ...) ## standardGeneric(&quot;summary&quot;) ## &lt;environment: 0x0000000015bedd60&gt; ## Methods may be defined for arguments: object ## Use showMethods(&quot;summary&quot;) for currently available ones. ## function (object, maxsum = 100L, ...) ## { ## nas &lt;- is.na(object) ## ll &lt;- levels(object) ## if (ana &lt;- any(nas)) ## maxsum &lt;- maxsum - 1L ## tbl &lt;- table(object) ## tt &lt;- c(tbl) ## names(tt) &lt;- dimnames(tbl)[[1L]] ## if (length(ll) &gt; maxsum) { ## drop &lt;- maxsum:length(ll) ## o &lt;- sort.list(tt, decreasing = TRUE) ## tt &lt;- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]])) ## } ## if (ana) ## c(tt, `NA&#39;s` = sum(nas)) ## else tt ## } ## &lt;bytecode: 0x000000001ed38670&gt; ## &lt;environment: namespace:base&gt; By understanding the principle of generic functions, we have completed a picture of S3 objects. The most important thing we have to adopt is that in this model functions are not part of the object itself, they are defined separately, and the link between the object and its “method” is only in the function name by which R “links” the generic function and that object. Although this principle is primitive and vulnerable to faults in the hands of inattentive developers, it is easy to use and very effective. Finally, let’s note that this approach is not entirely unique to the language R - similar principles can be found in other programming languages. It is specific to R in that the principle is used openly and almost exclusively. Objects and generic functions will be re-visited when we learn to create our own functions, which will enable us to create both the constructors of our objects and their generic functions. 5.3.3 Conclusion on S3 objects In short, the conclusions about S3 objects can be as follows: S3 objects function in in a simple, informal way - they are simply lists with the arbitrary value of class attributes Much of this is left to the responsibility of the programmer the S3 object methods are not encapsulated within the objects, but are designed “out of” objects in the form of generic functions S3 objects are not suitable for complex object models due to heavy model maintenance and large potential of errors Exercises Create a data frame with the following command: cities &lt;- data.frame(zipcode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), cityTax= c(18, 15, 10, 13, 10)) Add thie “taxLevel” which will be the ordinal factor variable with the levels “small”, “medium” and “high” depending on whether the percentage of tax is strictly smaller than 12, between 12 and 15 or strictly greater than 15. Use the ifelse command. Replace the loops in the next block with equivalent vectorized operations (for the second loop, review the sum function documentation). a &lt;- numeric() i &lt;- 1 while (i &lt;= 100) { a &lt;- c(a, i) i &lt;- i + 1 } total &lt;- 0 for (i in a) { if (i %% 2 == 0) total &lt;- total + i * i } print (total) Create a class object Block with the attributes height, width and depth equal to10, 20 and 30 respectively. Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["packages.html", "6 Packages, built-in functions and environments 6.1 Working with packages 6.2 Built-in functions 6.3 Environments Exercises", " 6 Packages, built-in functions and environments 6.1 Working with packages The standard R distribution comes with two collections of packages (called r-base and r-recommended) that contain a kind of “core” of the language R-set of elements sufficient for conducting standard types of data analysis using the programming language R. In addition, CRAN (Comprehensive R Archive Network) is a rich repository of additional packages for a wide variety of applications, from “repairing” the basic elements of the language R to strictly specialized packages for specific types of analyses. As is common practice in other programming languages, R uses a “package” or “library” system to logically organize already “programmed” collections of data, functions, and compiled code. When starting the R environment, certain packages are automatically loaded in memory, making their content available for use. The list of loaded packages can be obtained using the search function. Zadatak 6.1 - search path # Call the `search` function (no parameters) # and see which packages are loaded into the environment # Call the `search` function (no parameters) # and see which packages are loaded into the environment search() ## [1] &quot;.GlobalEnv&quot; &quot;package:broom&quot; &quot;package:caret&quot; ## [4] &quot;package:e1071&quot; &quot;package:car&quot; &quot;package:carData&quot; ## [7] &quot;package:Hmisc&quot; &quot;package:Formula&quot; &quot;package:survival&quot; ## [10] &quot;package:lattice&quot; &quot;package:sn&quot; &quot;package:stats4&quot; ## [13] &quot;package:gridExtra&quot; &quot;package:RSQLite&quot; &quot;package:hflights&quot; ## [16] &quot;package:lubridate&quot; &quot;package:GGally&quot; &quot;package:forcats&quot; ## [19] &quot;package:stringr&quot; &quot;package:dplyr&quot; &quot;package:purrr&quot; ## [22] &quot;package:readr&quot; &quot;package:tidyr&quot; &quot;package:tibble&quot; ## [25] &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; &quot;package:MASS&quot; ## [28] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; ## [31] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; ## [34] &quot;Autoloads&quot; &quot;package:base&quot; We see that most packages are listed as package::package_name. The layout of the package also represents their “priority” in the namespace traversal path, which will be discussed later. If we want to load a new package into our environment, we can do this by using the library function with the package name provided as parameter (without quotes). Zadatak 6.2 - loading packages in the working environment # load the `dplyr` package # load the `dplyr` package library(dplyr) The command from the previous example can have two outcomes: if the package exists on a local computer (in a folder set for additional packages), it will be loaded into the working environment. Package loading can be accompanied by messages about objects that are “masked” after loading. This in particular means that the new package has temporarily denied access to certain elements from previously loaded packages because their names match. This often does not pose any problems, but if the user has the need to access the masked elements they will have to use their “full name” - e.g. they will also have to specify the name of the package where they are located. For example, if the filter function of the stats package is masked after loading the new package, it is still available through the full name stats:: filter, but not directly through the namefilter, as this will call the function from the latest loaded package. More details about how R resolves the names of variables and functions will be given below in this lesson. If we do not have the above package on the local computer, we receive an error message that this package does not exist. In this case, it is necessary to retrieve the package from the CRAN repository using the install.packages function, which gives the name of one or more packages (with quotation marks!) as parameters. This function assumes that the R environment has a defined CRAN mirror ie. the specific address of the CRAN repository from where the package will be downloaded. A large number of countries have their own “copy” of the CRAN repository, but unfortunately, for unclear reasons, the Republic of Croatia no longer has its own CRAN repository, and at the time of writing this notebook there is no indication that the same will be established. If we are working in the RStudio interface, we have very likely placed the CRAN repository at the first startup (ie we have selected the Global option which our package installation requests automatically forward to the nearest CRAN repository), but if we did not do or work in another development then we need to look for a method of setting up a CRAN repository with the help of documentation, if we want to load additional packages. Zadatak 6.3 - installing a package from a CRAN repository # install the `dplyr` package from the CRAN repository # (You can do this even if you already have the specified package) # load the package again into the work environment # print the search path # install the `dplyr` package from the CRAN repository # (You can do this even if you already have the specified package) install.packages(&quot;dplyr&quot;) # load the package again into the work environment library(dplyr) # print the search path search() ## [1] &quot;.GlobalEnv&quot; &quot;package:broom&quot; &quot;package:caret&quot; ## [4] &quot;package:e1071&quot; &quot;package:car&quot; &quot;package:carData&quot; ## [7] &quot;package:Hmisc&quot; &quot;package:Formula&quot; &quot;package:survival&quot; ## [10] &quot;package:lattice&quot; &quot;package:sn&quot; &quot;package:stats4&quot; ## [13] &quot;package:gridExtra&quot; &quot;package:RSQLite&quot; &quot;package:hflights&quot; ## [16] &quot;package:lubridate&quot; &quot;package:GGally&quot; &quot;package:forcats&quot; ## [19] &quot;package:stringr&quot; &quot;package:dplyr&quot; &quot;package:purrr&quot; ## [22] &quot;package:readr&quot; &quot;package:tidyr&quot; &quot;package:tibble&quot; ## [25] &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; &quot;package:MASS&quot; ## [28] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; ## [31] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; ## [34] &quot;Autoloads&quot; &quot;package:base&quot; Note: as a rule, we install packages only once, through the console so that there is never a need to install the package installations in R Markdown documents; also, for easier organization of reports, loading all required packages is usually done by at the beginning of the document, in the code snippet called setup. We can notice that installing and loading packages will automatically acquire and load all the packages that are prerequisites for using the requested package, which greatly facilitates the user’s work, whu does not have to worry about what “extra” needs to be installed in order to use package features. If we want to find out more information about a package, we can also do this by using the library function with the help parameter set to the package name. library(help = dplyr) # recommendation: try directly in the console Another quite popular way of perusing documentation of the package is with the help of the so-called “vignettes”. Vignettes are actually “mini-tutorials” of a package done in HTML which present the functionality of the package in an accessible, easlly legible way with the help of detailed explanations and the associated program code. We can look at which vignettes are installed on the system by calling the browseVignettes() function without parameters (or optionally adding as the parameter the package name if we only care about its vignettes). If the package has only one vignette (for example, stringr), we can also open the vignette immediately with the help of the vignette option. vignette(&quot;stringr&quot;) # recommendation: try directly in the console 6.2 Built-in functions In previous chapters, we have already introduced some of the functions that we get together with our R distribution. These are, for example, numeric functions (log,abs, sqrt,round, etc.), vector creation functions (rep,seq, etc.), package functions (install.packages, library, etc.) and so on. R rarely uses the term “built-in” functions since - as it was already shown - the R environment automatically loads some commonly used packages whose elements are immediately available for use, without necessarily indicating the name of the package they are in. For example. the stats package contains a rich set of functions related to statistical processing. One of these functions is rnorm, which returns the numerical vector of the desired length whose elements are randomly selected from the normal distribution with arithmetic mean 0 and standard deviation 1 (these values can also be changed using themean and sd parameters). If we want, we can invoke this function using the package_name::function_name(parameters) syntax. Zadatak 6.4 - calling a package function # create a vector `x` that will have 10 random elements # drawn from standard normal distribution # use the full name of the `rnorm` function of the` stats` package # round the elements of vector `x` to two decimal places # use the full name of the `round` function from the `base` package # print vector `x` # create a vector `x` that will have 10 random elements # drawn from standard normal distribution # use the full name of the `rnorm` function of the` stats` package x &lt;- stats::rnorm(10) # round the elements of vector `x` to two decimal places # use the full name of the `round` function from the `base` package x &lt;- base::round(x, 2) # print vector `x` x ## [1] 0.04 0.11 1.43 0.98 -0.62 -0.73 -0.52 -1.75 0.88 1.37 Although this is a syntactically correct way of calling a function, R allows us to exclude package names and simply name the function directly. Zadatak 6.5 - removing the package reference # create vector `y` by the same principle as vector x # use only one line of code # use the function names without the name of the package # print vector `y` # create vector `y` by the same principle as vector x # use only one line of code # use the function names without the name of the package y &lt;- round(rnorm(10), 2) # print vector `y` y ## [1] -1.69 -0.63 0.02 0.71 -0.65 0.87 0.38 0.31 0.01 -0.04 We can ask ourselves - how does R know where the function we want to call is, if we did not explicitly load or specify a package containing that function? We will explain the exact reason in the next chapter dealing with the environmenst and already seen concept of the “search path”. Listingg all available functions, even those that are more commonly used, would be redundant since R language is best learned with specific application in mind, and after a certain time of use the user slowly creates his/her “own” collection of functions that represent his usual “analysis toolset”. In any case, it is recommended that you select one of the publicly available R reminders (reference cards or cheat sheets) which will then always be at hand during R programming. Some R reference cards are also available on the CRAN itself (it is enough to enter the CRAN reference card in the search engine and view the results), but here we will take the opportunity and recommend excellent cheat sheets on the pages of the RStudio interface available on the following link: https://www.rstudio.com/resources/cheatsheets/ (Alternatively, type RStudio cheat sheets into the search engine). These reminders cover the majority of useful things related to the general elements of the language R (Base R, Advanced R), but also specific packages that we will later learn (dplyr, ggplot2) and represent very valuable resources both for learning R and for long-term use. To easily follow the future lessons, we recommend that you print out the mentioned reminders and get acquainted with the elements they contain. Finally, let’s say that R allows us to quickly get help on the function by simply calling ? function_name or help(function_name), and that we can get examples of using the function through example(function_name). We should use these calls very often even if we think that we are well acquainted with the function we are calling - it is possible that there is an additional parameter (or a related function that is also often listed in the documentation), which will further help us in carrying out the task relted to the function we want to use. 6.3 Environments As already mentioned, work in R is usually related to managing different objects. In order to manage these objects at all, we need mechanisms to help us refer to the objects concerned. In R (and other programming languages), this is called “binding”. When we create the x variable of the numeric type and assign it the number 5, we actually created a (one-element) numerical vector and created a reference to that data using a character string x, which we can then use to acquire that data. Therefore, when we want to access some variables, R must search its internal “records” which state whichvariables currently exist, of what types they are and how to access them. In order to find the variable, R uses a mechanism called “lexical scoping”&quot; based on the concept of “environments”. The “environment” is often referred to as a “bag of names”. It helps us to logically group the names of the objects we use and to help R find the name in other environments if the variable does not exist in the current environment. The latter is enabled with the help of a fact that (almost) each environment has a parent environment. This system of parent-environment links creates a kind of “environment hierarchy”, often referred to as “search path”; R, looking for the default variable name, searches the environments “upwards” until it finds the first appearance of the requested name or encounters an the final environment (the so-called “empty environment” ). What is interesting is the fact that the environment itself is an object - we can create a reference to it, send it to functions, and so on. The “default” environment in which we work and in which we create new variables is the so-called “global environment”, or .GlobalEnv (watch out for the dot!). It is at the bottom of the environment hierarchy. We can get a reference to it via the mentioned name of the variable, or by using the globalenv() function. Zadatak 6.6 - global environment # create a variable `e` which refers to the global environment # print `e` # create a variable `x` and assign the number `5` to it # execute the `ls` function, without parameters # execute the `ls` function with `e` as a parameter # print `x` # print `e$x` (notice the list syntax!) # create a variable `e` which refers to the global environment e &lt;- .GlobalEnv # or e &lt;- globalenv() # print `e` e # create a variable `x` and assign the number `5` to it x &lt;- 5 # execute the `ls` function, without parameters #ls() # try directly on console! # execute the `ls` function with `e` as a parameter #ls(e) # try directly on console! # print `x` x # print `e$x` (notice the list syntax!) e$x ## &lt;environment: R_GlobalEnv&gt; ## [1] 5 ## [1] 5 From the last example, we can see that the environment also references itself, so this is completely correct (although unnecessarily complicated) syntax for printing the x variable: e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$x The environments are somewhat similar to lists, which are in fact a kind of “encapsulation” of a number of objects in a unique structure. The most important differences between the environment and the list are: the order of elements in the environment is irrelevant the environment (as a rule) has a link to the environment of the parent Let’s look at who the parent environment is to the global environment with the help of parent.env function. Zadatak 6.7 - parent environments # print out the parent environment of the global environment and explain the result # print out the parent environment of the global environment and explain the result parent.env(e) ## &lt;environment: package:broom&gt; ## attr(,&quot;name&quot;) ## [1] &quot;package:broom&quot; ## attr(,&quot;path&quot;) ## [1] &quot;C:/R/R-3.5.1/library/broom&quot; Slightly unexpectedly, the parent of the global environment is the last loaded package! This is not really unusual - the global environment has a “priority” in referencing the variable, but immediately below it are the objects and functions that we last loaded into the work environment (which suits us as the assumption holds that the “most recent” package is the one that we intend to use immediately). In other words, by loading a package, the new package is always “set” between the global environment and the package that was previously loaded last. When we called search function, we actually got the hierarchy of environments that represented loaded packages. This environmenthierarchy is the already mentioned “search path”. With the help of parent.env, we can determine what environment the environment will consider as a parent. In this way, we can make our own hierarchy of environments. Furthermore, the variables we create do not have to use references from the global environment (which is actually the basic function of the operator &lt;-), we can store them in any environment we want, but we need to use assign and get, or a combination of operators $ and &lt;-. Look at the example below to get the feeling how custom environments work. # example - a small hierarchy of custom environments e2 &lt;- new.env() e3 &lt;- new.env() # hierarchy `e3` --&gt; `e2` --&gt; `e` (global) parent.env(e2) &lt;- e parent.env(e3) &lt;- e2 # creating variable `x` in `e2` assign(&quot;x&quot;, 5, e2) # or e2$x &lt;- 5 # checking if there is an `x` in `e2` exists(&quot;x&quot;, e2) # printing `x` from `e2` get(&quot;x&quot;, e2) # or e2$x ## [1] TRUE ## [1] 5 Why use the environment in practice? The environment is a convenient way of “wrapping” a set of variables that we can then send together in a function - which is especially convenient if the relevant variables refer to some large datasets. As we will see in the next lesson, R does not support the so-called call-by-reference principle when sending objects to the function, but rather uses the so-called copy-on-modify mechanism. This means that the function will use the reference to the original object sent to it as a parameter up to the command that will try to change the object; at that moment a copy of that object is created and all the changes pertain to the copy. This can lead to a significant slowdown of programs with developers who are not familiar with this fact and who, for example, program a function that transforms the data frame. If the function instead of referring to the data frame sends a reference to the environment in which the “wrapped” frame is copied, then copying the variable will not occur because the environment is the only object for which copy-on-modify is not valid. We note that the special case of this method is to “send” the global environment to a function, which is actually reduced to the use of a “global variable” - something that is often avoided in other programming languages, but is not that uncommon in R. Finally, let’s demonstrate the attach function that analysts often use to speed up the analysis process, but which can cause problems if we are not careful with its use. This function will insert the data frame directly into the search path to allow us “easier” access to variables, and with potential accidental side effects. Let’s take a look at this example. Zadatak 6.8 - attach function cities &lt;- data.frame( zipcode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalary = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), tax = c(18, 15, 10, 13, 10)) # execute function `attach` with `cities` as parameter # do this only once! # print the search path and comment on the result # print the `zipcode` variable # change the third element from the `tax` variable to 12 # print `cities` # execute the `ls` function # use the `detach` function to remove `cities` from the search path # execute function `attach` with `cities` as parameter # do this only once! attach(cities) ## The following objects are masked _by_ .GlobalEnv: ## ## cityName, population ## The following object is masked from package:tidyr: ## ## population # print the search path and comment on the result search() cat(&quot;-------------------------\\n&quot;) # print the `zipcode` variable zipcode cat(&quot;-------------------------\\n&quot;) # change the third element from the `tax` variable to 12 tax[3] &lt;- 12 # print `cities` cities cat(&quot;-------------------------\\n&quot;) # execute the `ls` function #ls() # try it on console! # use the `detach` function to remove `cities` from the search path detach(cities) ## [1] &quot;.GlobalEnv&quot; &quot;cities&quot; &quot;package:broom&quot; ## [4] &quot;package:caret&quot; &quot;package:e1071&quot; &quot;package:car&quot; ## [7] &quot;package:carData&quot; &quot;package:Hmisc&quot; &quot;package:Formula&quot; ## [10] &quot;package:survival&quot; &quot;package:lattice&quot; &quot;package:sn&quot; ## [13] &quot;package:stats4&quot; &quot;package:gridExtra&quot; &quot;package:RSQLite&quot; ## [16] &quot;package:hflights&quot; &quot;package:lubridate&quot; &quot;package:GGally&quot; ## [19] &quot;package:forcats&quot; &quot;package:stringr&quot; &quot;package:dplyr&quot; ## [22] &quot;package:purrr&quot; &quot;package:readr&quot; &quot;package:tidyr&quot; ## [25] &quot;package:tibble&quot; &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; ## [28] &quot;package:MASS&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [31] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [34] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; ## ------------------------- ## [1] 10000 51000 21000 31000 2000 ## ------------------------- ## zipcode cityName avgSalary population tax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ------------------------- Let’s explain what happened in the example above. With the attach function, the cities data frame became a “mini-environment”, i.e. its columns became available within the search path. The obvious benefit of this is that we can refer to the columns directly, without referencing the original data frame and operator $. But this seemingly practical trick has hidden traps - first, if the column names match those of the global environment, then those columns will not be visible (we will be notified of this with an adequate warning). Second - and much more problematic - if we try to change the column of the data frame by directly referencing it, R will prevent it and will quietly apply the copy-on-modify principle by creating a new, global variable that will be a copy of the referenced column. An inattentive analyst can miss the fact that the changes are not reflected at the data frame itself, which can have far-reaching consequences. These potential problems are very widespread among R beginners, so in the R literature it is commonly suggested that the attach function is not used unless it is deemed very necessary. For example. Google’s R-style guide says “the error potentials for using the attach function are numerous, so avoid it” . If we really want to simplify our code and avoid repeating the name of the data frame every time we refer to one of its columns, it is recommended to use additional packages that are designed to facilitate the management of data frames. These packages will be introduced in some of the next chapters. Exercises Load the following packages in the working environment: magrittr, dplyr, tidyr,ggplot2. Print the search path and check where the loaded packages are. The following commands will create a vector of 20 randomly selected natural numbers from 1 to 100. # 20 random natural numbers from 1 to 100, with repetition set.seed(1234) a &lt;- sample(1:100, 20, replace = T) Use the cheat sheets and/or official documentation to find built-in functions that perform the following tasks. Print vector a the values of the vector a arranged in reverse order unique values from the vector a the values of the vector a sorted in ascending order We mentioned that loaded packages are actually “environments”. If we want to get a direct reference to them, we need to use as.environment and the package name. Try to get a reference to the package::magrittr package in the form of an environment, and use ls to check which names are contained in it. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["user.html", "7 User Defined Functions 7.1 How to define a function Exercises", " 7 User Defined Functions Although R is an object-oriented programming language, it leans heavily towards the realm of functional programming. This functional paradigm is not new (it dates back to the 1950s), but has recently gained considerable popularity as a kind of complement to object-oriented programming, which might be said was the dominant programming paradigm in the last few decades. In order not to delve too deep into the subject of functional programming and its relationship to object-oriented principles, we will list only a few indicative features and guidelines related to these two paradigms. Object-oriented programming in principle, sees the program as a system of nouns where the components are realized in the form of objects that encapsulate the attributes of the mentioned noun and the methods that perform certain tasks related to the given noun. Also, object-oriented systems focus on the controlled change of the components’ state as a result of the exchange of messages between them. Functional programming sees the program as system of verbs where the functions, ie the tasks we want to execute, have priority over the components over which these tasks are executed. Functional programming models the information system through components that generally do not change their own state, so that the result of the program is strictly dependent on the inputs, which facilitates testing and maintenance. In short, the distinction between object-oriented programming and function programming is often stated as follows: “For object-oriented programming, we create data that contains functions, and in fuctional programming we create functions containing data”. We do not have to be too bothered with the features of functional programming to learn R, nor should it be necessary to adopt a completely new programming paradigm. But for successful mastering of R, adapting some of the functional programming concepts can prove to be extremely useful since it will allow us to write a cleaner and more efficient code that will be in accordance with the way in which R is designed as a language. In R, the following is true: functions are “first-order” objects, we can reference them with a variable of the selected environment, send them to functions as arguments, receive as return values function and store them in data structures, such as a list. The function in R is simply an “executable” object. A large number of functions - especially those that replace the program loop construct - work on the principle of functional languages where we perform the work in a manner that declaratively specifies which function we want to apply on which data structure, and let the programming language itself perform low-level tasks such as iteration by structure and preparation of the result. Examples of this will be learned soon, and now let’s first get to know the syntax of user defined functions in R. 7.1 How to define a function In the general case, the definition of a new function looks like this: function_name &lt;- function(input arguments) { function_body return_statement } We can notice that in the function definition we use the operator &lt;-. This is not a coincidence - the definition of a function is nothing else than creating a `function ’object that we then associate with a particular variable; the name of the variable is actually the “name” of the function. In R we do not define the types of input and output arguments. Input arguments have the name and optional default value. The function formally returns one value, which is not necessarily restrictive if we want to return more values -we simply encapsulate them in the form of a vector or list. The return keyword is optional - the function returns the result of the last expression in the function, so it is often enough to specify only the variable that represents the return value as the last order of the function. Finally, if we want to increase the robustness of the function in such a way that we reject the execution of the logic within a function if certain conditions are not satisfied, we can use the stopifnot function. This function calculates the default logical expression and terminates the function if the specified condition is not true. Zadatak 7.1 - first user defined function # write the function `parallelMax` which requires two numeric vectors as input # and returns a vector of the same size containing the larger # between two corresponding elements of the original vectors # if one or both vectors aren&#39;t numeric or aren&#39;t the same size # the function must throw an error # do not use loops! # execute this new function over the following vector pairs # c(T, F, T) i c(1, 2, 3) # c(1, 2, 3, 4) i c(5, 6, 7) # c(1, 2, 3) i c(0, 4, 2) # (second part of the exercise should be tested inside the console!) # write the function `parallelMax` which requires two numeric vectors as input # and returns a vector of the same size containing the larger # between two corresponding elements of the original vectors # if one or both vectors aren&#39;t numeric or aren&#39;t the same size # the function must throw an error # do not use loops! parallelMax &lt;- function(a, b) { stopifnot(is.numeric(a) &amp;&amp; is.numeric(b) &amp;&amp; length(a) == length(b)); ifelse(a &gt; b, a, b) } When calling a function, we can optionally specify the parameter names, and R will even allow the mixing of named and unnamed parameters (although this is not something we should often use in practice). When R connects the sent values with formal parameters, the named parameters will have priority and will be resolved first, after which the unnamed parameters will be resolved in te order they were provided. We can see this in the next exercise, in which we will use the opportunity to showcase a very useful function - paste. This function concatenates character strings with the addition of a space separator (there is an alternative function paste0 for joining without spaces). Zadatak 7.2 - function parameters printABC &lt;- function(a, b, c) { print(paste(&quot;A:&quot;, a, &quot;B:&quot;, b, &quot;C:&quot;, c)) } # think before executing - what will be printed with the following command? printABC(1, a = 2, 3) printABC &lt;- function(a, b, c) { print(paste(&quot;A:&quot;, a, &quot;B:&quot;, b, &quot;C:&quot;, c)) } printABC(1, a = 2, 3) ## [1] &quot;A: 2 B: 1 C: 3&quot; In practice, we should stick to conventions to first use unnamed parameters, and then named. It is customary to set only those named parameters whose default value does not suit us, whereas strict ordering does not matter (although using the order provided in the function definition will increase the legibility of our code). If we want to write a function that receives an arbitrary number of arguments, we use the element ..., i.e. the ellipsis. An example of this feature is the built-in paste function which can receive an arbitrary number of character strings. If we use the ellipsis in our functions, we should place them at the end of the list of input arguments, and within the function itself we simply convert it into a list, and then access its parameters in the way that suits us. Zadatak 7.3 - function with an arbitrary number of parameters printParams &lt;- function(...) { params &lt;- list(...) for (p in params) print(p) } # call the above function with any random parameters printParams(c(1, 2, 3), 5, T, data.frame(x = 1:2, y = c(T, F))) ## [1] 1 2 3 ## [1] 5 ## [1] TRUE ## x y ## 1 1 TRUE ## 2 2 FALSE Finally, let’s recall the chapter in which we talked about S3 objects and the fact that R does not have a formal system for creating and using objects, but it is recommended that you use a separate constructor function that replaces the “manual” matching of the object and the declaration of its class. Now that we know how to create our own functions, we can look at how a potential constructor of the ‘Person’ class would look. # constructor of the `Person` class Person &lt;- function(id, surname, weight) { stopifnot(is.character(id)) stopifnot(is.character(surname)) stopifnot(is.numeric(weight) &amp;&amp; weight &gt; 0) p &lt;- list(id = id, surname = surname, weight = weight) class(p) &lt;- &quot;Person&quot; p } Let’s try to create a new object of the Person class with the help of this constructor. Zadatak 7.4 - constructor function # create `john`, a `Person` with the following characeristics: # id: 1357135713, surname: Watson, weight: 76 # print `john` john &lt;- Person(&quot;135135713&quot;, &quot;Watson&quot;, 76) john ## $id ## [1] &quot;135135713&quot; ## ## $surname ## [1] &quot;Watson&quot; ## ## $weight ## [1] 76 ## ## attr(,&quot;class&quot;) ## [1] &quot;Person&quot; The advantage of the constructor is additional robustness in the form of exactly defined attribute names, as well as the capabilities of embedding additional checks (eg id and surname must be character strings and the weight must be a number, etc.). If we create our own S3 objects, it is recommended that we define the corresponding constructor functions for them. 7.1.1 The “copy-on-modify” principle One of the more common questions raised when learning a new programming language is whether the functions work in “call-by-value” or “call-by-reference” mode. The difference is basically whether the function may change the content of the variables sent at the place of the formal argument or not; call-by-value principle forward only copies of original arguments. On the other hand, the call-by-reference principle makes it so the function receives “references” of the original variables, i.e. it behaves as if the original variables were passed to the function and all changes to them would be reflected in the calling function or program. The language R uses a hybrid principle known as copy-on-modify. With this principle, references are forwarded to the function, which allows us to transmit “large” variables without fear of unnecessary copying. But this is only valid if the function does not change the value of the resulting variables - at the moment when the function attempts to make any changes, copying the variable is carried out and the function continues to work on the copy. Because of this it is said that R as such does not support call-by-reference (one reason for introducing “reference class” objects, i.e. “RC object model” into language R, is precisely the introduction of this principle). Let’s check the above statements in a following example. # attempt to change variables from the calling environment f &lt;- function(x) { x &lt;- x + 1 } x &lt;- 5 f(x) print(x) ## [1] 5 The function actually creates a new temporary environment within which it stores “local” variables. In the above example, the f function introduces a new x variable that masks the external variable x so that all changes no longer reflect the value of the external variable. It is important to note that the function could access an external variable even without sending it to the function, since by referencing the x variable that does not exist in the local environment the R search function will continue in the parent’s environment, which in this case would be the global environment. Attempting to change this variable would still fail - R would detect an attempt to change the variable and create a local copy of the same name. Does this mean that a function can never change variables from the calling environment? Of course not. One way to do this is to send a reference to the environment within which the object we are changing exists (or, alternatively, let the function itself get a reference to the global environment if the variable is there). Zadatak 7.5 - changing global environment variables # implement function `f` which gets a reference to the global environment # and increases the &quot;outer&quot; `x` by 1 x &lt;- 5 # call `f(x)` and then print `x` # implement function `f` which gets a reference to the global environment # and increases the &quot;outer&quot; `x` by 1 f &lt;- function() { e &lt;- globalenv() e$x &lt;- e$x + 1 } x &lt;- 5 # call `f(x)` and then print `x` f() print(x) ## [1] 6 A simpler way of solving the above task would be using the &lt;&lt;- operator. This operator’s function is to change the variable of the given name that is located somewhere in the search path. R will follow the search path, and change the first occurrence of the specified variable. If the variable of this name does not exist anywhere in the search path, R will create a new variable in the first environment above the environment of the function. # operator `&lt;&lt;-` f &lt;- function(x) { x &lt;- 6 x &lt;&lt;- 7 } x &lt;- 5 f() x ## [1] 7 This operator is potentially unpredictable and we will achieve greater robustness using the assign or$operator with reference to the environment where the variable we want to change is. Finally, we must mention one feature of functions in R - the so-called. “lazy evaluation”. This simply means that R will not evaluate the received parameter until it is explicitly used. Up to that moment, this object is so-called. “promise” - R “knows” how to evaluate that object but it will not do so until it really needs it. This increases the efficiency of the language; if a parameter is used only in a conditional branch, then in scenarios when it is not needed it will not consume memory. But equally, we need to be careful because a lazy evaluation can lead to unexpected problems if we do not take into account its existence. 7.1.2 Function as an object We have already said that R has good support for the so-called “functional programming” which represents a programming paradigm that puts emphasis on designing functions without reliance on objects with interchangeable states. One of the characteristics of such languages are “first class functions”, which means that the language supports the definition of functions in such a way that they are equal objects to all other types of objects - they can be stored in a variable, used as an input argument of another function or as a return value, stored in other data structures, etc. Let us show this on a trivial example. We know that R offers the function sum within the base package, and this function calculates the arithmetic sum of the vector elements we send to it as an input parameter. But sum is actually the name of the variable that references the code that implements this function. If we want, we can very easily bind this function to some other variable by which we have effectively changed its name, or - better said - provided an alternative way of calling from a completely different environment. sum2 &lt;- sum sum2(1:10) # same as sum(1:10) This is easiest to understand in a way that the function is simply an “callable variable”, whereby the “call” refers to the use of a syntax that includes a reference to the function and input arguments framed in parentheses, which will return some value after execution in the R environment. The function can also be a return value from another function. funcCreator &lt;- function() { f &lt;- function(x) x + 1 return(f) } newFunc &lt;- funcCreator() # we get the &quot;add one&quot; function newFunc(5) ## [1] 6 The function simply creats a new function and returns it to the calling program as it would have done with any other object. The return value is stored in the variable that is now “callable” - if we add brackets and parameters it will be executed in the way it is defined within the function that created it. Note that we could use the fact that the function returns the result of the last expression and define the function even shorter: # shorter definition funcCreator &lt;- function() { function(x) x + 1 } These functions are often referred to as “factories” or “generators” of functions, and in contrast to the above example, in practice, the function generator often receives some parameters that determine how the returned function will behave. Try to create a function factory that returns the multiplication functions using a pre-set parameter. Zadatak 7.6 - function factory # create the `multiplicationFactory` function that creates # multiplication functions by the pre-set constant # use the above function to create the `times2` function # which doubles the received number # call the `times2` function with parameter 3 and print out the result # create the `multiplicationFactory` function that creates # multiplication functions by the pre-set constant multiplicationFactory &lt;- function(x) { function(a) a*x } # use the above function to create the `times2` function # which doubles the received number times2 &lt;- multiplicationFactory(2) # call the `times2` function with parameter 3 and print out the result times2(3) ## [1] 6 The multiplicationFactory function actually creates a “family” of functions that all provide the multiplication option with the selected number - i.e. parameter selected by the programmer itself. This way of managing functions may be initially confusing, but by using it in practice (which we will show in the next chapter), it is easy to notice the added flexibility and effectiveness of such an approach. If we define a function, and do not bind it to some variable, then we created the so-called “anonymous function”. # anonymous function function(x) x * x We can notice that each function is initially “anonymous”. If we return to the function definition syntax, we see that it is actually a combination of creating an anonymous function and binding it to a new variable. Of course, leaving the function anonymous as we did in the example above does not make much sense, just as it does not make sense to define a vector or list without creating a reference to that object - in that case the created object is not in any way usable because there are no links to it and will be quickly deleted by R within the “garbage collection” routine. We can ask ourselves - is there a scenario where the anonymous function is meaningful and useful? Explicit anonymous functions are used when a “disposable” function is needed, for example, as an argument for some other function. If the function we want to send as an argument is easy to define in a single line, and we do not plan to use it afterwards in the program, then it makes no sense to define it separately and assign it its own reference. An example of this will be seen in the apply family of functions. At the end of this section, we repeat the most important things - in R, the function is an object like any other, the only specificity is that it is an object that is “executable”, ie, which, using the function call syntax, does some work and returns some value. Even anonymous function can be executed (although only once, since we do not have a reference for future calls). # calling the anonymous function (function(x) x + 1)(2) ## [1] 3 7.1.3 Generic functions We have already mentioned generic functions in the chapter on objects, but let’s briefly recall what they are all about. The programming language R is not based on the so-called “message exchange principle” of the object-oriented languages, where, for example, the drawing of the graph might be performed like this: # the usual OOP principle invokes object methods graph.draw() but like this: # R just calls the method unto an object draw(graph) In the first case, a graph is an object that implements a special drawing method, and we have to call it in order to get the required graph image. In the second, there is an “external” function that “knows” how to draw a graph. This function is called a “generic function”. The properties of the generic function are as follows: the function has an intuitive, clearly defined purpose expected mode of operation is similar for multiple object types (eg drawing will always result in an image of sorts) each type of object requires its own implementation depending on the characteristics of the object (e.g. the way the method of drawing the circle differs from drawing a square) The method of implementing generic functions (for S3 objects!) is actually extremely simple, which is probably the reason for their wide acceptance and great popularity in the R community. The process is reduced to three simple steps: choose the name of a generic function (eg infoAbout) and declare that it is a generic function alternatively, choose one of the existing generic functions create an object and declare its class (eg Person) we implement the function called gen_function_name.class_name (e.g. infoAbout.Person) And that’s all! R does not require any additional steps, the above is quite sufficient for R to recognize the new generic function and apply it to all objects for whose class this generic function is implemented in the form gen_function_name.class_name (or gen_function_name.default for all classes for which there is no special implementation). In the next exercise, let’s try to implement the generic infoAbout method for the Person class. Zadatak 7.7 - new generic function peter &lt;- Person(id = &quot;12345678&quot;, surname = &quot;Parker&quot;, weight = 78) # create a new generic `infoAbout` function using the `UseMethod` function infoAbout &lt;- function(x) UseMethod(&quot;infoAbout&quot;) # implement a `infoAbout.Person` function which takes a `Person` # and writes the following on screen # ID: &lt;id&gt;, surname: &lt;surname&gt;, weight: &lt;weight&gt; # use the `paste` function to prepare the printout # and `cat` to put it on screen # implement a `infoAbout.default` function # which simply fowards the input parameter to the `cat` function # call `infoAbout` with `peter` as parameter # call `infoAbout` with `1:5` as parameter # implement a `infoAbout.Person` function which takes a `Person` # and writes the following on screen # ID: &lt;id&gt;, surname: &lt;surname&gt;, weight: &lt;weight&gt; # use the `paste` function to prepare the printout # and `cat` to put it on screen infoAbout.Person &lt;- function(p) { rez &lt;- paste(&quot;ID:&quot;, p$id, &quot;, surname:&quot;, p$surname, &quot;, weight:&quot;, p$weight, &quot;\\n&quot;) cat(rez) } # implement a `infoAbout.default` function # which simply fowards the input parameter to the `cat` function infoAbout.default &lt;- function(x) cat(x) # call `infoAbout` with `peter` as parameter infoAbout(peter) # call `infoAbout` with `1:5` as parameter infoAbout(1:5) ## ID: 12345678 , surname: Parker , weight: 78 ## 1 2 3 4 5 Of course, we did not necessarily have to create our own function to get the “print” functionality for our new class - it would probably be better to use already existing generic functions such as print or cat. Zadatak 7.8 - augmenting the existing generic functions # make sure `print` is a generic function # (print out its source code by referencing its name) # augmnet the `print` function so it allows pretty printing # of the `Person` class # (you may use the already created `infoAbout.Person` class) # call `print` with `peter` as parameter (or try the autoprint!) # make sure `print` is a generic function # (print out its source code by referencing its name) print # augmnet the `print` function so it allows pretty printing # of the `Person` class # (you may use the already created `infoAbout.Person` class) print.Person &lt;- infoAbout.Person # call `print` with `peter` as parameter (or try the autoprint!) print(peter) ## function (x, ...) ## UseMethod(&quot;print&quot;) ## &lt;bytecode: 0x000000001312e508&gt; ## &lt;environment: namespace:base&gt; ## ID: 12345678 , surname: Parker , weight: 78 Finally, we demonstrate the ability of R to list all the currently known implementations of a generic method. To do this we simply use the methods function to which we pass the name of the method concerned. With the same function we can also check which generic functions implementations exist for a particular class. For this we use the class parameter to which we are passing the class name for which we are interested in finding generic functions implemented for it. Zadatak 7.9 - methods function # list all implementations of the `summary` function # check with generic function implementations exist for the `factor` class # list all implementations of the `summary` function #methods(summary) # try on console! cat(&quot;-----------------------\\n&quot;) # check with generic function implementations exist for the `factor` class methods(class = &quot;factor&quot;) ## ----------------------- ## [1] - / [ [[ [[&lt;- ## [6] [&lt;- + all.equal Arith as.character ## [11] as.data.frame as.Date as.duration as.interval as.list ## [16] as.logical as.period as.POSIXlt as.vector as_date ## [21] as_datetime as_factor brief cbind2 coerce ## [26] Compare corresp droplevels fixed format ## [31] histogram initialize is.na&lt;- is_vector_s3 length&lt;- ## [36] levels&lt;- Logic Math Ops plot ## [41] print rbind2 recode relevel relist ## [46] rep scale_type show slotsFromS3 summary ## [51] Summary type_sum xtfrm ## see &#39;?methods&#39; for accessing help and source code Exercises R has a which function which converts a logical vector into a numeric one containing indexes where the original vector has a TRUE value (so c(T, F, F, F, F, T, F, T) becomes c(1, 6, 8)). Create a function which replicates this behaviour. Take the numerical vector x of length n. In statistics, the standardized moment of the k-th order is calculated like this: \\[\\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})}^{k+1}\\] Create a factory of moment functions (moment(k)) for calculating the standardized central moment of the k-th order. Create the functions zero_moment (x) and first_moment (x) with parameter values k set to 0 and 1 accordingly. Test the functions on vector 1:1000. Compare the results given by the sd (standard deviation) function over the vector 1: 1000 and root of the first moment you have calculated. Implement a constructor for the class Employee which inherits the Person class defined by the following constructor and print implemenation: Person &lt;- function(id, surname, weight) { p &lt;- list(id = id, surname = surname, weight = weight) class(p) &lt;- &quot;Person&quot; p } print.Person &lt;- function(p) { rez &lt;- paste(&quot;ID:&quot;, p$id, &quot;, surname:&quot;, p$surname, &quot;, weight:&quot;, p$weight, &quot;\\n&quot;) cat(rez) } Employee has all the attributes of the Person class as well as the superior attribute which represents a reference to a employee who is his/her superior (if such exists, otherwise it should be NA). Create two objects of the Employee class (one superior to another) and print them with the print function. Then implement your own version of the generic function print for the Employee class that prints employee data and his/her superior employee data (if it exists, otherwise it prints a message that there is no superior employee). Reprint both employees with the print function. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["apply.html", "8 ‘Apply’ family of functions 8.1 What are apply functions? 8.2 The apply function 8.3 The lapply, sapply and vapply functions 8.4 Other functions from the apply family and the available alternatives Exercises", " 8 ‘Apply’ family of functions 8.1 What are apply functions? Very often, knowledge of the basics of the language R is reflected by the skill of using the so-called apply family of functions, available in thebase package. These functions are specifically designed to perform repetitive tasks over various data structures, and as such they replace the program logic that would usually be realized in a through program loops. Additionally, these functions typically receive other functions as input arguments and, to a certain extent, encourage the functional programming paradigm. The family name comes from the fact that these functions commonly have a suffix “apply”. Some of the functions from this family are: apply lapply sapply vapply tapply,mapply, rapply … All of these functions work in a similar way - they receive a data set, a function that we want to apply to elements of that set, and optional additional parameters, and as the output give a set of function results, most often “packaged” in an appropriate format. The difference is mainly in the types of input and output arguments, as well as specific details about the implementation of the function itself and/or the way results are prepared. This family of functions is best learned through examples. We will begin with the “basic” function - apply. 8.2 The apply function The apply function is the only one that literally shares the name with the family of these functions. It is intended to work with matrices (actually with arrays, but since it is relatively rare to use data structures that have more than two dimensions, here we will focus only on matrices). The command syntax is as follows: result &lt;- apply( &lt;matrix&gt;, &lt;rows (1) or columns (2)&gt;, &lt;function_name&gt; ) Or, described in words, to implement the apply function, we: choose a matrix decide whether to “cut it” by rows or columns declare which function we want applied to each row (or column) Depending on how function works, as a result we get a matrix or (which is a more frequent case) a vector. Let’s try to use this function in a concrete example. Zadatak 8.1 - the apply function m &lt;- matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE) # print matrix `m` # use the `apply` function to calculate # and print the column sums of the `m` matrix # use the `apply` function to calculate # and print the multiplicaton of row elements # from the `m` matrix # print matrix `m` m cat(&quot;------------\\n&quot;) # use the `apply` function to calculate # and print the column sums of the `m` matrix apply(m, 2, sum) cat(&quot;------------\\n&quot;) # use the `apply` function to calculate # and print the multiplicaton of row elements # from the `m` matrix apply(m, 1, prod) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## ------------ ## [1] 12 15 18 ## ------------ ## [1] 6 120 504 If we want to perform a custom task to the rows/columns, we often use an anonymous function, for example: apply(m, 1, function(x) x[1]) # return the first element of each row Zadatak 8.2 - the apply function and anonymous functions # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the `apply` function # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the `apply` function apply(m, 1, function(x) round(log(sum(x)),2)) ## [1] 1.79 2.71 3.18 Let’s repeat - apply (and related functions) implicitly * disassembles the input data structure into elements. In the examples above, these elements - rows or columns - are actually numeric vectors. The argument x received by an anonymous function is exactly that vector, or, better said each of these vectors* that are sent one by one. The results of the function are “remembered” and “packed” into the final result. Let’s try to program the last example without using the apply function. Zadatak 8.3 - loop as the alternative to the apply function # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the for program loop # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the for program loop rez &lt;- numeric(nrow(m)) for (i in 1:nrow(m)) rez[i] &lt;- round(log(sum(m[i,])), 2) rez ## [1] 1.79 2.71 3.18 If we compare the syntax of the examples with and without using the apply function, we can see how much the syntax which uses apply s actually “cleaner”. If we use loops, we must explicitly specify the logic of passing through the structure, which draws attention away from the job description that we actually want to do. What if we want to send to the apply function a function which needs several parameters? For example, let’s say that instead of the upper function that extracts the first line element we want a function with two parameters - the first a vector (matrix row or column), and the second an integer that indicates the index of the element to extract. The answer is simple - just add additional parameters at the end of the function call. # apply function and input function with multiple parameters apply(m, 1, function(x,y) x[y], 2) # second element of each row Finally, it should be noted that for similar processing of data in the matrix form, we do not necessarily need to use apply - many popular operations such as adding row or column elements, calculating the average of the elements of the rows and columns, and the like. This has already been implemented through functions such as rowSums,colSums, rowMeans,colMeans and the like. They are easier to use, but specialized - for more flexibility, the most common option is apply. 8.3 The lapply, sapply and vapply functions The name of the lapply function comes from list apply - i.e. apply the function to the elements of lists. To put simply - it is a function that will receive the list and a function as the input arguments, apply the functions to each individual list element and return again a new list as a result. Zadatak 8.4 - the lapply function l &lt;- list(a = 1:3, b = rep(c(T, F), 10), c = LETTERS) # use the `lapply` function to calculate the length (number of elements) # of each element of the `l` list # use the `lapply` function to calculate the length (number of elements) # of each element of the `l` list lapply(l, length) ## $a ## [1] 3 ## ## $b ## [1] 20 ## ## $c ## [1] 26 Just like with the apply function, for thelapply function, we often use anonymous functions as a parameter. The following task has no special practical use, but it will help us understand the functionality of the lapply function in combination with a slightly more complex anonymous function. Zadatak 8.5 - the lapply function and anonymous functions # process the elements of the `l &#39;list as follows: # - Calculate the mean value if it is a numerical vector # - count the values of TRUE if it is a logical vector # - calculate the length of the vector for all other cases # use the `lapply` function and an anonymous function # do not forget that anonymous function can also use blocks! # process the elements of the `l &#39;list as follows: # - Calculate the mean value if it is a numerical vector # - count the values of TRUE if it is a logical vector # - calculate the length of the vector for all other cases # use the `lapply` function and an anonymous function # do not forget that anonymous function can also use blocks! lapply(l, function(x) { if (is.numeric(x)) { mean(x) } else if (is.logical(x)) { sum(x) } else length(x) }) ## $a ## [1] 2 ## ## $b ## [1] 10 ## ## $c ## [1] 26 The lapply function is essentially quite simple to use and is very popular due to this fact. But once we use it for a while, we can find it irritating that t it always returns the list as a result, although some other data structure would be more suitable for us - for example a vector, especially if the resulting list has just simple numbers as elements. For this reason, R offers the unlist function to simplify the list to a vector. Zadatak 8.6 - the unlist function l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # calculate the mean value of the elements of the `l` list # print the results as a numeric vector # use `lapply` and `unlist` # calculate the mean value of the elements of the `l` list # print the results as a numeric vector # use `lapply` and `unlist` unlist(lapply(l, mean)) ## a b c ## 5.5 15.0 150.0 The displayed combination of lapply andunlist will give us as a result a one-dimensional vector, which in many cases is what we want. But sometimes some other data structure would suit us - for example, a matrix. In this case we need an additional step in transforming a one-dimensional vector into a matrix using the matrix function, with the number of rows and columns being explicitly assigned. The question may arise - why is lapply not able to check the structure of the result it has created and determine the optimal data structure for formatting it (vector, matrix, or list)? That’s exactly the idea behind the sapply function, or simplified list apply. This function first performs lapply internally, and then simplifies the result to a vector, matrix or array, depending on the characteristics of the results obtained. Zadatak 8.7 - the sapply function l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # calculate the median of elements of the `l` list # and collect the results in a numeric vector # use the `sapply` function # extract the first and last element of each of the elements of the `l` list # use `sapply` and anonymous function # calculate the median of elements of the `l` list # and collect the results in a numeric vector # use the `sapply` function sapply(l, median) cat(&quot;------------\\n&quot;) # extract the first and last element of each of the elements of the `l` list # use `sapply` and anonymous function sapply(l, function(x) c(x[1], x[length(x)])) ## a b c ## 5.5 15.0 150.0 ## ------------ ## a b c ## [1,] 1 10 100 ## [2,] 10 20 200 Note that as a result in the last example, we received a matrix, but that R formed it “by columns”. If we wanted a matrix with elements arranged in rows, we can not use sapply for this directly, because the matrix is formed internally, without the possibility of forwarding thebyrow = T parameter. To obtain such a matrix, one option is already mentioned with the combination of lapply,unlist and matrix, or - more simply - transposing thesapply results using t function (from transpose). The sapply function is quite popular due to its simplicity and efficiency, so it is relatively often used in interactive analysis. On the other hand, the use of this function in program scripts is not recommended since its result is unpredictable in the general case - e.g. the script can expect a matrix in the continuation of the code, and the sapply function, due to the specificity of the input data, returns the vector, which can cause unforeseen results, which is not easy to spot later and diagnose where the error occurred. If we are developing our own programs in R and want to use sapply, then the better choice will be thevapply function, which works identically to sapply, but uses an additional parameter called FUN.VALUE with which we explicitly define what kind of “simplification” we expect. For example. numeric(3) means that the result of applying the function to each element of the original list should be a numeric vector of three elements. If the result for any list item differs from the expected one, the function will raise an error. Zadatak 8.8 - the vapply function myList &lt;- list(numbers &lt;- c(1:5), names &lt;- c(&quot;Ivo&quot;, &quot;Pero&quot;, &quot;Ana&quot;), alphabet &lt;- LETTERS) # think which of the following calls will be successful, # and which will throw out the error # check the results on the console vapply(myList, length, FUN.VALUE = numeric(1)) vapply(myList, function(x) as.character(c(x[1], x[2])), FUN.VALUE = character(2)) vapply(myList, function(x) as.logical(x), FUN.VALUE = character(1)) Finally, let’s return briefly to lapply and consider one important fact - it is intended for use on lists, and data frames are actually lists. In other words, the lapply function is very handy for processing tabular datasets when we want to apply a particular function to the columns of the data frame. One of the more frequent operations performed in data analysis is the so-called. “normalization” of the numeric columns of the data frame - i.e. reducing all numerical values to “normal” distribution with the arithmetic mean of 0 and standard deviation of 1. This can be done by reducing each individual value by the arithmetic mean of the column (the mean function) and dividing with standard deviation of the column (function sd). This is a great way to demonstrate the use of lapply with data frames. Zadatak 8.9 - the lapply function and data frames df &lt;- data.frame( a = 1:10, b = seq(100, 550, 50), c = LETTERS[1:10], d = rep(c(T,F), 5), e = -10:-1) # normalize numerical columns using `lapply` # do not change the remaining columns # round the normalized values to three decimal places # save the result in the df variable # print df # normalize numerical columns using `lapply` # do not change the remaining columns # round the normalized values to three decimal places # save the result in the df variable df &lt;- lapply(df, function(x) { if (is.numeric(x)) { round((x - mean(x))/sd(x), 3) } else x }) # print df df ## $a ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 ## ## $b ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 ## ## $c ## [1] A B C D E F G H I J ## Levels: A B C D E F G H I J ## ## $d ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE ## ## $e ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 We see that after using lapply we get a list and that if we want the result in the form of a data frame we need to add another step using theas.data.frame function. If we are looking for a simpler way that immediately gives the data frame as a result, there is one convenient “trick” that we will explain below. Let’s look at the solution of the previous problem, and put the following little change in the assignment of the result of the lapply function: df[] &lt;- lapply(...) In this way, R will not create a “new” variable named df, but rather thelapply result will be entered in the ’all rows and columns of the df data frame. This made it possible for us to get the result in the form of a data frame, which we actually wanted. For this very reason, in R scripts, we will often see a similar syntax (df [] &lt;- lapply ...). Try to modify the above example in the above manner and make sure that the result will be a data frame. Another commonly used trick in working with data frames is the following command: sapply(df, class) This command actually gives us the answer to the question - which types are the columns of the given data frame? Although there are other ways to get this information, this method is popular both because of the compactness of the results and the independence of additional packages. 8.4 Other functions from the apply family and the available alternatives In the previous chapters, we have probably listed the most popular members of the apply family. This family has more members, including some who do not have a suffix -apply: mapply, which works in parallel over multiple data structures rapply, which recursively applies functions within the structure tapply, which applies functions over sub-groups within a structure defined by factors Map, themapply version, which does not simplify the result by, thetapply version for data frames etc. The reason why these functions will not be explained in detail is twofold: firstly, as already mentioned, these functions are in practice applied much less often than the functions we have shown in the previous chapters. Secondly, with the increase in popularity of the language R, a large number of packages are oriented to improve the existing functions of the language R in the sense of easier and more efficient programming, especially when working with data frames. If we are looking for convenient alternative functions to those from the apply family, it’s recommended to look at some of the following packages plyr - an extremely popular package that, among other things, offers a number of functions very similar to apply functions, but derived in a way that they have a consistent signature and explicitly defined input and output formats that are easily read from the function name itself (in particular, the first letters ); so the llply function uses a list as both the input and the output, whilemdply needs a matrix as input and outputs a data frame purrr - a package that replaces the functions of theapply family with functions corresponding to similar functions from other programming languages; since the application of the same function to a number of elements of a data structure in functional languages is often called “mapping”, the set of functions of this package carries the prefix maps_, and the function names often correspond to the expected results (for examplemap2_lgl means that as a result we expect a logical vector , and the map2_df a data frame) dplyr - a relatively new package, which in a certain sense represents the successor of the plyr package but oriented almost exclusively toward data frames; the functions of this package are not so much oriented to replace the apply family functions as providing a specific platform for working with data frames in a manner similar to languages oriented precisely for this purpose, such as, for example, the SQL language In future lectures we will introduce the dplyr package precisely because this package greatly facilitates and accelerates the data analysis process and is extremely well accepted in the R community. Exercises Take the m matrix created by the following command: m &lt;- rbind(1:5, seq(2, 10, 2), rep(3, 5), 3:7, seq(100, 500, 100)) With the apply function and the new anonymous function, create a vector that will contain the first even element of each row, or zero if the corresponding row does not have even elements. The following commands will create a list of 100 elements where each element will be a numeric vector of a random length of 1 to 10. set.seed(1234) myList &lt;- replicate(100, sample(1:10, sample(1:10, 1))) With the help of lapply / sapply (and additional commands if necessary), create: the numerical vector v with the lengths of the list elements list l with normalized numerical vectors of the original list numerical vector ind4 with indexes of all list elements containing number 4 the df5 data frame containing columns which have all the elements of the length of 5 from the original list Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["pipe.html", "9 Pipeline operator and tidy data - UNDER CONSTRUCTION", " 9 Pipeline operator and tidy data - UNDER CONSTRUCTION Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["dates.html", "10 Dates and strings - UNDER CONSTRUCTION", " 10 Dates and strings - UNDER CONSTRUCTION Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["dplyr.html", "11 Managing data frames with dplyr package - UNDER CONSTRUCTION", " 11 Managing data frames with dplyr package - UNDER CONSTRUCTION Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["ggplot2.html", "12 Visualising data with ggplot2 package - UNDER CONSTRUCTION", " 12 Visualising data with ggplot2 package - UNDER CONSTRUCTION Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["regression.html", "13 Linear regression analysis - UNDER CONSTRUCTION", " 13 Linear regression analysis - UNDER CONSTRUCTION Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],
["predictive-Modeling.html", "14 Introduction to Predictive Modeling 14.1 What is predictive modeling? 14.2 Creating training and test datasets 14.3 Classification Predictive Models - kNN Classification 14.4 Package caret and predictive modeling", " 14 Introduction to Predictive Modeling 14.1 What is predictive modeling? In the previous chapter, we have shown that with the help of linear regression we can investigate and model the interplay of two (or more) variables. If there is collinearity between the two observed variables, then their relation can be modeled with the help of a simple straight line equation, which then allows to estimate the value of the second variable from the known value of one of the variables. This is the basic idea of so-calle. “predictive modeling” - a process which relies on familiar inputs and developed predictive models to get information about unknown outputs, i.e. goals. Better models provide better, or more accurate results. Or, as defined by Kuhn and Johnson in his book “Applied Predictive Modeling”: Predictive modeling is a process of developing a mathematical tool or model that is able to create accurate predictions. Predictive modeling has numerous usage options in various domains eg: predicting the quantity of sold products or expected profits identification of users planning to cancel the subscription disease diagnostics real estate evaluation estimation of movies’s profits on opening weekened spam detection etc. When developing predictive models, it is very important to: have adequately prepared, high quality input data choose a suitable method for creating a model properly conduct the evaluation and validation process The role of analysts in this process is to conduct all the steps of analysis thoroughly and to avoid common traps and errors. Namely, predictive models can ultimately have poor performance for a number of reasons - poorly prepared data, poorly conducted validation, creating models which “overfit” the training data, etc. In this chapter, we will get acquainted with some of the basic guidelines we must adhere to during predictive modeling. We will also familiarize ourselves with R language packages that allow us to leave complex steps to the computer, that is, to approach predictive modelling from high-level perspective while the computer independently performs low-level data preparation, modeling, and validation steps. 14.2 Creating training and test datasets When we created linear models in the previous chapter, after the creation of the model, we looked at summarized model information with the help of summary function - estimation of predictor and target relationship, the value of residual standard error, the value of adjusted R-squared measure etc. This information has enabled us to evaluate the quality of developed models. But in the whole process we ignored one key thing - all of this information was based on data used to create the model itself. In other words, we have gained insight into how well a model works over known data, which the model has already seen during the model’s creation. As a rule, it is much more important to evaluate how well a model works over unknown data, i.e. to estimate the quality of predictions once the model gets completely new data. We say that we want models that generalize well, i.e. models that have learned generic features of the domain entities described in theobtained data, and not just the specifics of the dataset used to develop the model itself. This latter phenomenon is called “overfitting”. Very often, at the time of creating a model, we only have one dataset that we should also for training and model evaluation. As we have already said, using the same data for both purposes does not give us enough good information about how the model generalizes. A common procedure in this case is to split the initial set into two parts - a training set and a test set. How do you decide which observations to assign to which set? How many observations should be put into training, and how many in the testing set? As a rule, the training set and test set should have enough observations so that the obtained results could be statistically relevant so assuming a sufficiently large input dataset, splitting in two equal parts could be a satisfactory solution. However, in practice, we usually reserve more observations for training than for testing, so it is customary to use a 70:30 spluit, so 70% observations go into the training set, and the rest go in the test set. As for the procedure of selecting observations (i.e. “sampling”), there are several common procedures: random selection stratified random selection (ensuring equal representation of certain categories in both sets) timestamp-based selection (if the time component is key, i.e. it is important to estimate how well past information predicts future events) Random selection is a most commonly used method while more sophisticated sampling methods can be chosen if there is a clear need for them based on the goals of the analysis. Let’s now apply this knowledge to the development of a linear regression model by comparing its effectiveness on the training set, and then on the test data. For this we will use a new set of data related to the characteristics and quality of wine, wines.csv. This set was created by adapting the “Wine Quality Data Set” from the UCI Machine Learning Repository, available at[this link] (https://archive.ics.uci.edu/ml/datasets/wine+quality). In the next exercise, we’ll load this dataset and make some adjustments - we’ll categorize columns that obviously contain a category variable and remove rows with missing values. Lines with missing values often require a little more attention and a more sophisticated approach than simple removal, especially with larger amounts of missing values or potential additional information that these values denote. But in our case there are very few such rows, and when applying certain predictive modeling methods, they can create problems, so we will simply remove them. One of the quick ways to do this is by using the complete.cases function, which returns the indexes of all rows which do not have any missing value for the given data frame. Zadatak 14.1 - wine quality dataset # load data from the` wines.csv` file # in a variable called `wine` # examine the loaded data frame # categorize columns as needed # and remove rows with missing values # load data from the` wines.csv` file # in a variable called `wine` # examine the loaded data frame # categorize columns as needed # and remove rows with missing values wine &lt;- read.csv(&quot;wines.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) glimpse(wine) wine$type &lt;- factor(wine$type) wine &lt;- wine[complete.cases(wine),] ## Observations: 6,497 ## Variables: 13 ## $ fixed.acidity &lt;int&gt; 7, 63, 81, 72, 72, 81, 62, 7, 63, 81, 81,... ## $ volatile.acidity &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32,... ## $ citric.acid &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16,... ## $ residual.sugar &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00... ## $ chlorides &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050,... ## $ free.sulfur.dioxide &lt;int&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 1... ## $ total.sulfur.dioxide &lt;int&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132... ## $ density &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0... ## $ pH &lt;int&gt; 3, 33, 326, 319, 319, 326, 318, 3, 33, 32... ## $ sulphates &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47,... ## $ alcohol &lt;dbl&gt; 88, 95, 101, 99, 99, 101, 96, 88, 95, 11,... ## $ quality &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7,... ## $ type &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;white&quot;, &quot;white&quot;, &quot;whit... Suppose the attribute quality is the target variable and all the other columns are potential predictors. How do you split this set on the training set and test set using random selection? There are several ways to do this, and even specialized functions and packages for this purpose, but we will learn a simple and easy-to-understand method that uses the sample function. If we assume that df is the data frame that we want to split into a training and test set that we will store in df.train and df.test variables, then random selection can be done as follows: train_size &lt;- 0.7 * nrow(df) %&gt;% round # about 70% train_ind &lt;- sample(1:nrow(df), train_size) # indexes of training observations df.train &lt;- df[train_ind, ] df.test &lt;- df[-train_ind, ] Let’s apply this to our wines dataset and then train a linear regression model. Zadatak 14.2 - Splitting the input dataset and training a model set.seed(1234) # split the `wine` dataframe into `wine.train` and `wine.test` # using a random selection method and 70:30 ratio # train a linear regression model using the `wine.train` set # store the model in a variable called `linMod` # target variable is `quality` and all other variables are predictors # check the summary for the obtained model set.seed(1234) # split the `wine` dataframe into `wine.train` and `wine.test` # using a random selection method and 70:30 ratio train_ind &lt;- sample(1:nrow(wine), 0.7 * nrow(wine) %&gt;% round) wine.train &lt;- wine[train_ind, ] wine.test &lt;- wine[-train_ind, ] # train a linear regression model using the `wine.train` set # store the model in a variable called `linMod` # target variable is `quality` and all other variables are predictors # check the summary for the obtained model linMod &lt;- lm(quality ~ ., data = wine.train) summary(linMod) ## ## Call: ## lm(formula = quality ~ ., data = wine.train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1979 -0.5375 -0.0454 0.5078 4.9346 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.723e+01 4.451e+00 21.843 &lt; 2e-16 *** ## fixed.acidity -4.744e-04 4.133e-04 -1.148 0.251125 ## volatile.acidity -1.351e+00 1.011e-01 -13.366 &lt; 2e-16 *** ## citric.acid 3.106e-01 9.469e-02 3.280 0.001047 ** ## residual.sugar 1.648e-03 4.832e-04 3.411 0.000654 *** ## chlorides -3.657e+00 4.188e-01 -8.733 &lt; 2e-16 *** ## free.sulfur.dioxide 1.041e-04 2.445e-04 0.426 0.670234 ## total.sulfur.dioxide 1.090e-04 9.830e-05 1.109 0.267442 ## density -9.150e+01 4.484e+00 -20.404 &lt; 2e-16 *** ## pH -7.765e-05 1.259e-04 -0.617 0.537511 ## sulphates 8.632e-01 9.224e-02 9.358 &lt; 2e-16 *** ## alcohol 3.983e-16 3.991e-16 0.998 0.318384 ## typewhite -3.714e-01 4.514e-02 -8.229 2.44e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7853 on 4533 degrees of freedom ## Multiple R-squared: 0.1851, Adjusted R-squared: 0.183 ## F-statistic: 85.82 on 12 and 4533 DF, p-value: &lt; 2.2e-16 We see that the adjusted R-squared measure is relatively low and the average error is quite high, so linear regression is perhaps not the best option this scenario (or maybe the obtained characteristics of the wines are simply not good enough to predict its quality). In spite of this, we will now check how well the model works on unseen observations. In order to check the quality of our model, we have to choose the criterion to which we will adhere. For numeric goals we often use the RMSE measure (root mean square error): \\[RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n}(\\widehat{y}_{i} - y_{i})}{n}}\\] where n is the number of observations,\\(\\widehat{y}_{i}\\) prediction of the observation i and \\(y_ {i}\\) actual value of the target variable of that observation. Although there are packages that contain this function, we can easily program it ourselves. Zadatak 14.3 - Function for calculating the RMSE measure # create a `rmse` function that will use prediction vector # and a vector of actual target vales as parameters # and calculate the RMSE measure according to the above formula # create a `rmse` function that will use prediction vector # and a vector of actual target vales as parameters # and calculate the RMSE measure according to the above formula rmse &lt;- function(pred, act) sqrt(mean((pred - act) ** 2)) Let us now add prection columns to wine.train andwine.test datasets and then calculate the value of the RMSE measure for both. Zadatak 14.4 - Model evaluation for the training and test set # add a `predQualityLR` column to `wine.train` and `wine.test` # using the `predict` function and the `linMod` model # print the value of the RMSE measure for both sets # remove the `predQualityLR` column from the `wine.train` dataset # add a `predQualityLR` column to `wine.train` and `wine.test` # using the `predict` function and the `linMod` model wine.train$predQualityLR &lt;- predict(linMod, wine.train) wine.test$predQualityLR &lt;- predict(linMod, wine.test) # print the value of the RMSE measure for both sets rmse(wine.train$predQualityLR, wine.train$quality) rmse(wine.test$predQualityLR, wine.test$quality) # remove the `predQualityLR` column from the `wine.train` dataset wine.train$predQualityLR &lt;- NULL ## [1] 0.7841708 ## [1] 0.8078178 We see that the RMSE for the training set roughly corresponds to the residual standard error obtained in the model summary (a small difference is a result of the fact that we used the total number of observations, while in the calculation of the residual standard error we used the number of degrees of freedom - “dependent” variables, i.e. predictors). What is interesting is the fact that the value of RMSE measure of the test set is not dramatically larger than the RMSE measure, which means that the model works almost equally well (or badly) on new data. 14.3 Classification Predictive Models - kNN Classification The linear regression method allowed us to “guess” the numerical target variable. It is reasonable to ask the question - can we build a predictive model that will try to estimate the value of a categoric variable? This is an extremely important goal of predictive modeling since many domains have problems with determining the value of a particular category (the patient is ill or not, the device is corrupt or correct, the transaction is regular ora result of fraud, the client will default on the loan or not etc.) These are so-called “classification” problems for which - just as for the “regression” problems which use numerical goals - there is an extremely large set of developed methods. Very often, similar methods can be used for both purposes (sometimes with certain adjustments), so for example even if linear regression is not specifically suited for classification problems, its related “logistic regression” method is a very effective and popular approach to such a type of problem. In this section, we will focus on another, very popular classification method, which is very intuitive and easy to understand, but which allows us to inspect a number of interesting predictive modeling elements that we have not mentioned so far. It is a method called “k nearest neighbors”, or “kNN classification”. This method works in a very simple way. If we do not know the category of some observation, we simply find a number of observations which are the “closest” to that new observation. We then look at their categories, and then by majority vote determine which category to assign to the new observation. Let’s try to visualize this. Zadatak 14.5 - Visualization of the main idea for the method of kNN classification # create a scatterplot for `wine.train` dataset # put `chlorides` on `x` axis # and `volatile.acidity` on `y` axis # color point based on wine type # set the transparency of the points to 0.5 # create a scatterplot for `wine.train` dataset # put `chlorides` on `x` axis # and `volatile.acidity` on `y` axis # color point based on wine type # set the transparency of the points to 0.5 ggplot(wine.train, aes(chlorides, volatile.acidity, col = type)) + geom_point(alpha = 0.5) On the graph we can clearly see how the points form “clusters” of the same colors in certain areas. If we take a new observation regarding the wine of an unknown type, but the known values of the measures set on the x and y axes, by looking at its immediate neighborhood, we could conclude which type of wine the new wine belongs to. Observations deep within one of the “clouds” are very likely to belong to the type shown (although we see that there are exceptions!). On the other hand, observation within the “boundary” areas is much more likely to be misdiagnosed, and the different choice of neighboring numbers could result in different classification results. kNN classification is based on the concept of distance. Although there are different options for distance selection, we often rely on the so-called “Euclidean distance”, which is easily presented in the two- and three-dimensional Cartesian system by the shortest path between two points, and we can easily calculate its value using their coordinates and Pythagorean theorem. This distance is easily applied to n-dimensional spaces, so although we can not easily visualize points in a space whose dimension corresponds to the number of predictors, we can still easily calculate the value of the Euclidean distance between points. So, the way kNN classification works can be easily described as follows: the training set itself represents “domain knowledge”, that is, the predictive model itself for each new observation, we find k closest observations from the training set and assign the category to it by using the majority vote Although this process is relatively simple, there are some common questions which require answers. Firstly - how do we select the parameter k? Secondly, do we need to make some additional preparatory actions on the data set before we perform the kNN classification? Let’s deal with the second question first. Examine the graph we have drawn, more precisely its coordinate axes. We can see that the lengths of the axes do not necessarily scale to their numerical values equally, i.e. the unit interval on the x axis is not necessarily equal to the unit interval on y axis. This is normal and expected, since the values on axes do not necessarily have to use the same scale nor even the measuring unit. But there is one problem - when we use Euclidean distance, it treats all axes equally, which means that variables with larger ranges will automatically gain greater importance (e.g. the maximum value of the chlorides is around 0.611 while the maximum value of sulfur dioxide variable reaches over 3000!) Here we see the importance of data pre-processing . For kNN classification, normalization of numeric variables is recommended, i.e. the transformation of numeric variables in such a way that we deduct their average and divide them by standard deviation, which brings them all to the same scale. This process is somewhat more complex than it seems, because we have to make sure that new observations are also normalized in the same way. This means that we need to make sure both training and test numerical variables need to come from the same distribution. If we have enough data and both training and test observations are representative, then their means and standard deviations should be close enough so we can easily normalize each set independently. If we have small test sets, then the normalization of the test set should be done by remembering using the mean and standard deviation of the training set. To simplify this process somwhat, we will return to the original wine dataset and simply normalize the numeric columns beforehand, and then store it in the wineNorm variable (we can pretend that we knew means and standard deviations of populations of these variables beforehand so we used them to normalize both training and test set independetly). Then, we will split this set into sets of wineNorm.train andwineNorm.test analogously to the previous procedure. `r zadHead(“Normalization of numeric variables of the input dataset”) # normalize all numeric columns of the `wine` data frame # store the result in the `wineNorm` variable # use the `train_ind` object to split `wineNorm` # into `wineNorm.train` and `wineNorm.test` # normalize all the numeric columns of the `wine` data frame # store the result in the `wineNorm` variable wineNorm &lt;- lapply(wine, function(x) { if(is.numeric(x))(x - mean(x)) / sd(x) else x })%&gt;% as.data.frame # use the `train_ind` object to split `wineNorm` # into `wineNorm.train` and `wineNorm.test` wineNorm.train &lt;- wineNorm[train_ind,] wineNorm.test &lt;- wineNorm[-train_ind,] Let’s go back to the problem of selecting the value of the k parameter. How to pick the right value? Generally speaking, this problem is called “choosing a hyperparameter of a model,” since the model besides input data requires some additional input parameters to perform its function. Usually, when no mathematical method of calculating optimal hyperparameters exist, the only option we have is training models with various combinations of hyperparameters and finally selecting those hyperparameter which result in models showing the best performance. In this case, we often need another (third) part of the original dataset, usually called a “validation” dataset, which represents an additional step before testing the set in which we select the values of the hyperparameters for the “final” model. Specifically, for kNN classification: we train a larger number of models for different parameter values k over the training set with the help of the validation set, we find the model that has the best performance and choose its k we use chosen k and the entire training and validation set to train the model we make a final evaluation on the test set Below we will use a simplified version of this process which uses only the training and test set while setting the k parameter arbitrarily to 5. We will leave the entire process of possibly finding a better hyperparameter as an optional exercise. Likewise, as we will see at the end of this chapter, we often do not manually program instruction for finding best hyperparameters, but rather use high-level functions which allow us to only set things up declaratively, without having to deal with low-grade details about the implementation of the process itself. Let’s try to see if we can properly classify wine as “white” or “red” with the help of variables describing its chemical composition. We will use the kNN method, with the number of neighbors set to 5. Predictors will be all available variables except quality andtype. For kNN classification the base R offers the knn function . For our needs we will use the knn3 function from thecaret package. The function knn3 expands the basic knn function in a way that allows us to call it by following the standard programming conventions we have already learned when training linear regression models: model &lt;- selected_method(formula, training_dataset, additional_parameters) predictions &lt;- predict(model, test_dataset, additional_parameters) In the case of the knn3 function, an additional parameter in training the model is k, set to 5. When creating predictions, we will set the type parameter to class, meaning we want to predict the class itself (alternative is probs, returning predicted probabilities for each class). Let’s try to create a kNN model with the help of wineNorm.train and then find wine type predictions for wineNorm.test. Zadatak 14.6 - kNN Classification # create the variable `kNN5Mod` which will be # the result of calling `knn3` over the `wineNorm.train` set # target variable is `type` # predictors are all other variables except `quality` # add a `predictedType` column to `wineNorm.test` # which will store a result of calling the `predict` function # with `kNN5Mod` as a model and `wineNorm.test` as new data # set the `type` parameter to `class` # create the variable `kNN5Mod` which will be # the result of calling `knn3` over the `wineNorm.train` set # target variable is `type` # predictors are all other variables except `quality` #library(caret) # if needed kNN5Mod &lt;- knn3(type ~ . - quality, data = wineNorm.train, k = 5) # add a `predictedType` column to `wineNorm.test` # which will store a result of calling the `predict` function # with `kNN5Mod` as a model and `wineNorm.test` as new data # set the `type` parameter to `class` wineNorm.test$predictedType &lt;- predict(kNN5Mod, wineNorm.test, type = &quot;class&quot;) Note that we did not look for a model summary since we can not get too much information in this case. The kNN classifier can not provide us with some aggregated information about “learned” knowledge, it is just a “map of the domain space” that is then used for each new observation to determine which category it belongs to. How do we check the classifier’s performance? A typical procedure (with binary classifiers) is the creation of a so-calle “confusion matrix”. This simply means that we will create a table that will show how well the predicted values match the actual values. The easiest way to create this table is simply to call the table function with the prediction column and the actual value column as parameters. Zadatak 14.7 - Simple Configuration Matrix # print a confusion matrix by calling the `table` function # over the appropriate columns of the `wineNorm.test` dataset # print a confusion matrix by calling the `table` function # over the appropriate columns of the `wineNorm.test` dataset table(wineNorm.test$predictedType, wineNorm.test$type) ## ## red white ## red 476 14 ## white 9 1450 Looking at the results we can intuitively conclude that in this case the classifier works extremely well, that is, white and red wines can be very easily classified by looking at their chemical properties. But we often want to describe the quality of the classifier using an objective, numerical measure. There are a number of such measures, and most of them can be directly calculated using values from the confusion matrix. Specifically, if we call the confusion matrix cells TP, TN, FP, FN (true positive, true negative, false positive, false negative), where we treat one class as “positive” and hence assigning the names of the corresponding cells depending on whether the classifier correctly guessed the class (main diagonal) or not (side diagonal). In this case, from the confusion matrix we can directly calculate the following measures: accuracy: \\(\\frac{TP + TN}{TP + FP + TN + FN}\\) sensitivity (recall): \\(\\frac{TP}{TP + FN}\\) precision: \\(\\frac{TP}{TP + FP}\\) false positive rate/false negative rate: \\(\\frac{FP}{FP + TN}\\) ; \\(\\frac{FN}{TP + FN}\\) etc. These are just some of the possible measures. Although accuracy may be the most logical choice (because we actually get a percentage of correctly guessed observations), we often have to be careful because it can give us a distorted picture of the classifier’s effectiveness, especially in when there is a huge disbalance in categories or when one type of error is far more dangerous than the other. A typical example is the diagnosis of rare diseases - if the disease occurs in only 0.1% of cases, then the trivial classifier, which for all observations diagnoses that the disease is not present, works well in 99.9% cases. Also, if it is a dangerous disease, then FP error (the disease is diagnosed although not present) is far less important than FN errors (the disease is present but is not recognized). In these cases, selecting another measure (eg “sensitivity” or “false negative rate”) is often a much better quality indicator of the classifier. We can very easily manually calculate all these measures using base R. However, the confusionMatrix function from the caret package (which also leverages the e1071 package) gives us the same result as the table function but with the added convenience of computing a large number of measures that can help us evaluate the classifier’s quality. Zadatak 14.8 - confusionMatrix function # create a variable called `confMat` variable # which will stort the result of the `confusionMatrix` function # using the appropriate columns of the `wineNorm.test` dataset as parameters # print the `confMat` variable # create a variable called `confMat` variable # which will stort the result of the `confusionMatrix` function # using the appropriate columns of the `wineNorm.test` dataset as parameters #library(e1071) # if needed #library(caret) # if needed confMat &lt;- confusionMatrix(wineNorm.test$predictedType, wineNorm.test$type) # print the `confMat` variable confMat ## Confusion Matrix and Statistics ## ## Reference ## Prediction red white ## red 476 14 ## white 9 1450 ## ## Accuracy : 0.9882 ## 95% CI : (0.9823, 0.9925) ## No Information Rate : 0.7512 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.9685 ## Mcnemar&#39;s Test P-Value : 0.4042 ## ## Sensitivity : 0.9814 ## Specificity : 0.9904 ## Pos Pred Value : 0.9714 ## Neg Pred Value : 0.9938 ## Prevalence : 0.2488 ## Detection Rate : 0.2442 ## Detection Prevalence : 0.2514 ## Balanced Accuracy : 0.9859 ## ## &#39;Positive&#39; Class : red ## Since the variable confMat is an S3 object, with the help of the unlist function and the selection of the desired element, we can easily obtain only the numeric value of the measure we are interested in. This is useful if we want to integrate this function in our programming scripts. 14.4 Package caret and predictive modeling In the previous chapter we were already introduced to the caret package, more specifically some of its functions that help us with predictive modeling. This package actually offers a lot more than we have seen. Specifically, the caret package provides a set of tools to effectively perform all elements of the predictive modeling process: splitting data pre-processing of data feature selection adjusting the model with the help of re-sampling variable importance estimation As the dplyr package actually changes the way we use the R language to manage the data frames, so thecaret package enables a thorough modification of the predictive modeling approach used when programming in R. The functions of the caret package not only provide a cleaner syntax for low-level jobs, they also give the possibility of leveraging high-level approach for predictive modeling, where we declare declarative calls for what we want to do, and let R do low-level jobs returning us the corresponding result. Details of this package can be found at [this link] (https://topepo.github.io/caret/index.html), and below we will only give you a brief insight into some of the most useful features of this package. To demonstrate the declarative nature of predictive modeling with this package, we will look at two functions: train andtrainControl. The train function is actually a generic interface to a large number of predictive models (a list of all the models currently supported by the function can be [found here] (http://topepo.github.io/caret/train-models-by-tag.html) . In a large number of cases, the this function call is not different from the call of the predictive modeling method functions we have learned so far, specifically lm andknn3. The biggest difference is that instead of calling a specific function, here we define the method of predictive modeling using the method parameter. Let’s try to train a linear regression model using the train function and the previously prepared wine.train dataset. Zadatak 14.9 - train function and linear regression # using the `train` function from the `caret` package # train a linear regression model using the `wine.train` dataset # target variable is `quality` and all other variables are predictors # set the `method` `&quot;lm&quot;` # store the resuling model in a variable called `linMod` # using the `train` function from the `caret` package # train a linear regression model using the `wine.train` dataset # target variable is `quality` and all other variables are predictors # set the `method` `&quot;lm&quot;` # store the resuling model in a variable called `linMod` linMod &lt;- train(quality ~., data = wine.train, method = &quot;lm&quot;) # read the summary of the obtained model summary(linMod) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1979 -0.5375 -0.0454 0.5078 4.9346 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.723e+01 4.451e+00 21.843 &lt; 2e-16 *** ## fixed.acidity -4.744e-04 4.133e-04 -1.148 0.251125 ## volatile.acidity -1.351e+00 1.011e-01 -13.366 &lt; 2e-16 *** ## citric.acid 3.106e-01 9.469e-02 3.280 0.001047 ** ## residual.sugar 1.648e-03 4.832e-04 3.411 0.000654 *** ## chlorides -3.657e+00 4.188e-01 -8.733 &lt; 2e-16 *** ## free.sulfur.dioxide 1.041e-04 2.445e-04 0.426 0.670234 ## total.sulfur.dioxide 1.090e-04 9.830e-05 1.109 0.267442 ## density -9.150e+01 4.484e+00 -20.404 &lt; 2e-16 *** ## pH -7.765e-05 1.259e-04 -0.617 0.537511 ## sulphates 8.632e-01 9.224e-02 9.358 &lt; 2e-16 *** ## alcohol 3.983e-16 3.991e-16 0.998 0.318384 ## typewhite -3.714e-01 4.514e-02 -8.229 2.44e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7853 on 4533 degrees of freedom ## Multiple R-squared: 0.1851, Adjusted R-squared: 0.183 ## F-statistic: 85.82 on 12 and 4533 DF, p-value: &lt; 2.2e-16 We see that we get the same result as when we called the lm function directly at the beginning of this chapter. Note that in this case we haven’t used a large number of parameters of the train function that we can see in the documentation. For example, using the preprocess parameter can automatically perform some data preparation procedures such as normalization, BoxCox transformation, imputation of missing values, and so on. Let’s create a little more complex predictive model now. First, let’s examine a “control object” we can create using a function called trainControl. This function provides us with a “control panel” which allows us to fine-tune all parameters related to the training of our predictive model. Some of these parameters relate to so-called “data re-sampling” - this means that we can get a better estimate of the behavior of developed predictive models if we perform a process called “cross-validation”, where the training set is split multiple times and then a model is trained over and over again, always using a separate holdover part as the test set. In this way we actually get a number of models, each with their own results, which give us information not only about the quality but also the stability of the model. Additionally, we have the option of tweaking additional parameters, such as which summary funcation we want to apply on the resulting model, or whether we want the model (in the case of classification) to return the probabilities or just the resulting category. When finally create such a “control object”, we can recycle it as much as we want in the future steps of predictive modeling without the need to enter a large number of training-related parameters every time. A simple control object can look like this: # We use repeated cross-validation with 5 repeats ctrl &lt;- trainControl(method = &quot;repeatcv&quot;, repeats = 5) ## Warning: `repeats` has no meaning for this resampling method. Finally, let’s try to use two slightly more advanced methods of predictive modeling - the random forest method (the ranger method from the package of a same name) and the support vector method (thesvmRadial method from the kernlab) package. We will not go deeper into the details of these methods, just focus on how to call them with the help of the trainCtrl andtrain functions. In the following code we will also leverage a function called expand.grid. This function requires a vector different values of the hyperparameters we have provided, and will result in a dataframe containing all combinations of these hyperparameters. It is most commonly used in conjunction with the tuneGrid parameter to assign the candidate sets for predictive model hyperparameter - in this way, the train function will try out all combinations of the default parameters and (in conjunction with the cross validation method) select the parameters that showcase the best performance for the final model. #library(ranger) # if needed #library(kernlab) # if needed # we will use the same control object for both models # set `verboseIter` to TRUE # for better insight into training speed! ctrl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 2, verboseIter = FALSE) # random forest model rfMod &lt;- train(quality ~., data = wine.train, method = &#39;ranger&#39;, tuneLength = 10, trControl = ctrl, num.trees = 10) # support vector model # we use a grid of hyperparameters # and pre-process data with normalization svm.grid &lt;- expand.grid(C = c(2, 4, 8), sigma = c(0.25, 1, 2)) svmMod &lt;- train(quality ~., data = wine.train, method = &quot;svmRadial&quot;, trControl = ctrl, tuneGrid = svm.grid, preProcess = c(&quot;center&quot;, &quot;scale&quot;)) Finally, we make a simple evaluation of the model with the help of RMSE measures. Zadatak 14.10 - Easy Model Evaluation # with the help of the `predict` function and` rfMod` and `svmMod` models # add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset # print the value of the RMSE measure for all obtained models # with the help of the `predict` function and` rfMod` and `svmMod` models # add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset wine.test$predQualityRF &lt;- predict(rfMod, wine.test) wine.test$predQualitySVM &lt;- predict(svmMod, wine.test) # print the value of the RMSE measure for all obtained models cat(&quot;RMSE Linear regression:&quot;, rmse(wine.test$predQualityLR, wine.test$quality)) cat(&quot;\\nRMSE Random Forest:&quot;, rmse(wine.test$predQualityRF, wine.test$quality)) cat(&quot;\\nRMSE Support Vectors:&quot;, rmse(wine.test$predQualitySVM, wine.test$quality)) ## RMSE Linear regression: 0.8078178 ## RMSE Random Forest: 0.6772888 ## RMSE Support Vectors: 0.7011601 Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "]
]
